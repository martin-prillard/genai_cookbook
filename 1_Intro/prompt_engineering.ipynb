{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b9d8db3",
   "metadata": {},
   "source": [
    "# Practical introduction to *Prompt Engineering* and the impact of inference parameters\n",
    "\n",
    "Objectives:\n",
    "- Handle *prompt engineering* techniques (clear instructions, role/persona, constraints, *few-shot*, structured format, self-checking).\n",
    "- Experiment with the effect of inference parameters (temperature, top_p, top_k, penalties, max_tokens, seed) on variability, accuracy and style.\n",
    "- Set up a small loop for evaluating and plotting results.\n",
    "\n",
    "> ⚠️ **Keys/API and models**: by default, this tutorial uses the API of a compatible provider (e.g. OpenAI) via the environment variable `OPENAI_API_KEY`.  \n",
    "> You can use any *chat/completions* model (e.g. `gpt-4o-mini`, `gpt-4.1`, `o4-mini`...) or adapt the `LLMClient` class for another provider (OpenRouter, vLLM, etc.).\n",
    "\n",
    "## Lesson plan\n",
    "1. Preparing the environment and API\n",
    "2. Basic *prompting* techniques (instructions, roles, constraints)\n",
    "3. *Few-shot prompting\n",
    "4. Structured output (JSON + validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf3b47",
   "metadata": {},
   "source": [
    "## 1) Preparing the environment and LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332aced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Backend: OpenAI\n",
      "Client prêt.\n"
     ]
    }
   ],
   "source": [
    "import os, time, random, statistics, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from jsonschema import validate, ValidationError\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "@dataclass\n",
    "class GenParams:\n",
    "    model: str = \"gpt-4o-mini\"   # par défaut côté OpenAI; côté Ollama on substituera si nécessaire\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 1.0\n",
    "    max_tokens: int = 300\n",
    "    frequency_penalty: float = 0.0\n",
    "    presence_penalty: float = 0.0\n",
    "    seed: Optional[int] = None   # supporté par Ollama via options.seed et par OpenAI (selon modèles)\n",
    "\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"\n",
    "    Client LLM bi-backend:\n",
    "      - OpenAI via SDK et clé API\n",
    "      - Ollama via HTTP (déjà lancé avec `ollama serve`)\n",
    "    Sélection via la variable d'env USE_OLLAMA=1|true|yes\n",
    "    \"\"\"\n",
    "\n",
    "    use_ollama = False\n",
    "    ollama_base = \"http://localhost:11434\"\n",
    "\n",
    "    def __init__(self):\n",
    "        if self.use_ollama:\n",
    "            print(f\"⚙️ Backend: Ollama ({self.ollama_base})\")\n",
    "            # test rapide de santé (facultatif, mais pratique)\n",
    "            try:\n",
    "                r = requests.get(f\"{self.ollama_base}/api/tags\", timeout=3)\n",
    "                r.raise_for_status()\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(\n",
    "                    f\"Ollama semble indisponible sur {self.ollama_base}. \"\n",
    "                    f\"Assurez-vous que `ollama serve` tourne. Détails: {e}\"\n",
    "                )\n",
    "            self.client = None\n",
    "        else:\n",
    "            print(\"⚙️ Backend: OpenAI\")\n",
    "            api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise RuntimeError(f\"Renseignez votre clé API dans {api_key}.\")\n",
    "            if OpenAI is None:\n",
    "                raise RuntimeError(\"Le paquet 'openai' n'est pas installé. Faites: pip install --upgrade openai\")\n",
    "            self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # ---------- Implémentation Ollama (HTTP) ----------\n",
    "    def _ensure_ollama_model(self, model: str) -> str:\n",
    "        \"\"\"\n",
    "        Si l'utilisateur a laissé un modèle OpenAI (ex: gpt-4o-mini) alors qu'on est en mode Ollama,\n",
    "        on substitue un modèle local raisonnable.\n",
    "        \"\"\"\n",
    "        if not model or model.startswith(\"gpt-\"):\n",
    "            return \"mistral\"  # à ajuster selon vos modèles téléchargés: e.g. \"qwen2.5:7b\"\n",
    "        return model\n",
    "\n",
    "    def _chat_ollama(self, system: str, user: str, params: GenParams) -> str:\n",
    "        url = f\"{self.ollama_base}/api/chat\"\n",
    "        model = self._ensure_ollama_model(params.model)\n",
    "\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            # Mapping des options vers Ollama\n",
    "            \"options\": {\n",
    "                # temperature / top_p / seed sont supportés par Ollama\n",
    "                \"temperature\": params.temperature,\n",
    "                \"top_p\": params.top_p,\n",
    "                **({\"seed\": params.seed} if params.seed is not None else {}),\n",
    "                # Ollama n'a pas `max_tokens`, mais `num_predict`\n",
    "                \"num_predict\": params.max_tokens if params.max_tokens is not None else -1,\n",
    "                # Pas d'équivalents directs pour presence/frequency_penalty\n",
    "            },\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            r = requests.post(url, json=payload, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            # Réponse typique: {\"message\": {\"role\":\"assistant\",\"content\":\"...\"} , ...}\n",
    "            msg = data.get(\"message\", {})\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            return content.strip()\n",
    "        except requests.HTTPError as e:\n",
    "            try:\n",
    "                err = r.json()\n",
    "            except Exception:\n",
    "                err = r.text\n",
    "            raise RuntimeError(f\"Erreur HTTP Ollama: {e} | {err}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Echec appel Ollama: {e}\")\n",
    "\n",
    "    # ---------- Implémentation OpenAI (SDK) ----------\n",
    "    def _chat_openai(self, system: str, user: str, params: GenParams) -> str:\n",
    "        messages = []\n",
    "        if system:\n",
    "            messages.append({\"role\": \"system\", \"content\": system})\n",
    "        messages.append({\"role\": \"user\", \"content\": user})\n",
    "\n",
    "        rsp = self.client.chat.completions.create(\n",
    "            model=params.model,\n",
    "            messages=messages,\n",
    "            temperature=params.temperature,\n",
    "            top_p=params.top_p,\n",
    "            max_tokens=params.max_tokens,\n",
    "            frequency_penalty=params.frequency_penalty,\n",
    "            presence_penalty=params.presence_penalty,\n",
    "            seed=params.seed,\n",
    "        )\n",
    "        return rsp.choices[0].message.content\n",
    "\n",
    "    # ---------- API publique ----------\n",
    "    def chat(self, system: str, user: str, params: GenParams) -> str:\n",
    "        if self.use_ollama:\n",
    "            return self._chat_ollama(system, user, params)\n",
    "        return self._chat_openai(system, user, params)\n",
    "\n",
    "\n",
    "# ---------- Paramètres par défaut ----------\n",
    "DEFAULT_PARAMS = GenParams(\n",
    "    model=\"gpt-4.1-nano\",  # si USE_OLLAMA=1, on substituera automatiquement par \"llama3.1\"\n",
    "    temperature=0.7,\n",
    "    top_p=1.0,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "client = LLMClient()\n",
    "print(\"Client prêt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f167e4",
   "metadata": {},
   "source": [
    "## 2) Basic *prompting* techniques\n",
    "\n",
    "We're going to compare several variants for the same simple task (e.g. summarizing a short article).  \n",
    "Experiment: **without instructions**, **with constraints**, **with role/persona**.\n",
    "\n",
    "**Exercise:** Fill in the cells to formulate different prompts and compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad32077",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_text = (    \"Le *prompt engineering* consiste à concevoir des instructions claires pour guider un modèle de langage. \"\n",
    "    \"Il inclut des techniques comme le rôle, les contraintes de format, le few-shot, et la calibration des paramètres d'inférence. \"\n",
    "    \"Son objectif est d'améliorer la fiabilité, la structure et l'utilité des réponses.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230c5dc-dfb5-41d5-afa8-bdcd8256da28",
   "metadata": {},
   "source": [
    "### 2.a No instruction (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7403ef-b66d-4de5-9f28-02fbd9166e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline ===\n",
      " Bien sûr ! Voici une version légèrement reformulée de ton texte pour plus de clarté et de fluidité :\n",
      "\n",
      "Le *prompt engineering* consiste à élaborer des instructions précises afin de guider efficacement un modèle de langage. Cela inclut des techniques telles que l'attribution de rôles, l'imposition de contraintes de format, l'utilisation du few-shot learning, ainsi que la calibration des paramètres d'inférence. L'objectif est d'améliorer la fiabilité, la cohérence et l'utilité des réponses générées par le modèle. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_no_instruction = \"Peux-tu m'aider avec ce texte ?\"\n",
    "out_no_instruction = client.chat(system=\"\", user=f\"{p_no_instruction}\\n\\nTexte: {task_text}\", params=DEFAULT_PARAMS)\n",
    "print(\"=== Baseline ===\\n\", out_no_instruction, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b619574-e1a8-4479-945b-86cad0a3568b",
   "metadata": {},
   "source": [
    "### 2.b Clear instructions + measurable objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "304e2457-be67-41ec-92f8-16413e6d2b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Instruction claire ===\n",
      " Le *prompt engineering* vise à créer des instructions précises pour optimiser les réponses des modèles de langage en utilisant différentes techniques. En bref, cette pratique permet d'améliorer la qualité, la cohérence et la pertinence des résultats générés par l'intelligence artificielle. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_instruction = \"\"\"\n",
    "Résume le texte en **2 phrases maximum**, en français.\n",
    "N'utilise **aucune** liste à puces.\n",
    "Ajoute une phrase de conclusion commençant par \"En bref,\".\n",
    "\"\"\"\n",
    "out_instruction = client.chat(system=\"\", user=f\"{p_instruction}\\n\\nTexte: {task_text}\", params=DEFAULT_PARAMS)\n",
    "print(\"=== Instruction claire ===\\n\", out_instruction, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f7145-bbb7-4fb1-aed7-d0a0fc3cf84d",
   "metadata": {},
   "source": [
    "### 2.c Role / persona + constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13561bb3-8d80-4501-8639-7d8f70ceee80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Persona + contraintes ===\n",
      " PROMPT ENGINEERING : OPTIMISATION DES INSTRUCTIONS POUR LES MODÈLES DE LANGAGE\n",
      "\n",
      "Le *prompt engineering* désigne l'art de créer des instructions précises afin d'orienter efficacement un modèle de langage. Il intègre diverses techniques telles que l'attribution d’un rôle spécifique, l'imposition de contraintes de format, l’utilisation de l’approche few-shot, et la calibration des paramètres d’inférence. Ces méthodes visent à renforcer la fiabilité, à structurer les réponses, et à maximiser leur utilité. En élaborant des prompts soigneusement conçus, l’utilisateur peut guider le modèle pour qu’il produise des résultats plus pertinents et cohérents. Cette discipline est essentielle pour exploiter pleinement le potentiel des modèles de langage, notamment dans des applications variées telles que l’assistance, la génération de contenu ou l’analyse de données. En résumé, le prompt engineering permet d’optimiser la communication avec l’intelligence artificielle en améliorant la qualité et la précision des réponses. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p_role = \"\"\"\n",
    "Vous êtes un assistant pédagogique concis et rigoureux.\n",
    "Ton: neutre, précis.\n",
    "Tâche: reformule le texte en **150 mots max** et ajoute un **TITRE EN MAJUSCULES** en première ligne.\n",
    "\"\"\"\n",
    "out_role = client.chat(system=\"\", user=f\"{p_role}\\n\\nTexte: {task_text}\", params=DEFAULT_PARAMS)\n",
    "print(\"=== Persona + contraintes ===\\n\", out_role, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a980e",
   "metadata": {},
   "source": [
    "## 3) *Few-shot prompting*\n",
    "\n",
    "Showing the model 2-3 examples of inputs/outputs can guide it towards the expected style.\n",
    "\n",
    "**Exercise:** create 2 examples of *paraphrase* and ask the model to produce a third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59c95438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'apprentissage par renforcement emploie des récompenses pour orienter l'agent.\n"
     ]
    }
   ],
   "source": [
    "fewshot_examples = [\n",
    "    {\"input\": \"Le chat dort sur le canapé.\", \"output\": \"Le félin sommeille sur le sofa.\"},\n",
    "    {\"input\": \"Ce modèle fait des erreurs occasionnelles.\", \"output\": \"Ce système commet parfois des inexactitudes.\"},\n",
    "]\n",
    "\n",
    "instruction = \"Paraphrase la phrase en conservant le sens et un ton neutre.\"\n",
    "\n",
    "def build_fewshot_prompt(examples, new_input):\n",
    "    lines = [instruction, \"\", \"Exemples:\"]\n",
    "    for ex in examples:\n",
    "        lines.append(f\"- Input: {ex['input']}\")\n",
    "        lines.append(f\"  Output: {ex['output']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"Nouvelle phrase: {new_input}\")\n",
    "    lines.append(\"Donne uniquement la paraphrase en français.\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "user_input = \"L'apprentissage par renforcement utilise des récompenses pour guider l'agent.\"\n",
    "prompt = build_fewshot_prompt(fewshot_examples, user_input)\n",
    "out_fewshot = client.chat(system=\"\", user=prompt, params=DEFAULT_PARAMS)\n",
    "print(out_fewshot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcb39479-a614-4d9b-b455-2dbb1084f00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Paraphrase la phrase en conservant le sens et un ton neutre.\\n'\n",
      " '\\n'\n",
      " 'Exemples:\\n'\n",
      " '- Input: Le chat dort sur le canapé.\\n'\n",
      " '  Output: Le félin sommeille sur le sofa.\\n'\n",
      " '- Input: Ce modèle fait des erreurs occasionnelles.\\n'\n",
      " '  Output: Ce système commet parfois des inexactitudes.\\n'\n",
      " '\\n'\n",
      " \"Nouvelle phrase: L'apprentissage par renforcement utilise des récompenses \"\n",
      " \"pour guider l'agent.\\n\"\n",
      " 'Donne uniquement la paraphrase en français.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baafa7ce",
   "metadata": {},
   "source": [
    "## 4) Structured output (JSON) + validation\n",
    "\n",
    "**Exercise:** force a **JSON** output respecting a minimal schema and validate it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60b24ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"titre\": \"Classification supervisée\",\n",
      "  \"mots_cles\": [\"apprentissage automatique\", \"classification\", \"étiquettes\", \"données annotées\", \"modèle\"],\n",
      "  \"resume\": \"La classification supervisée consiste à apprendre une fonction associant des entrées à des étiquettes à partir d'exemples annotés.\" \n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt_json = \"\"\"\n",
    "Produis une synthèse **au format JSON strict** du texte ci-dessous.\n",
    "Champs requis:\n",
    "- \"titre\": string\n",
    "- \"mots_cles\": array de 3 à 5 strings\n",
    "- \"resume\": string (<= 60 mots)\n",
    "Réponds **uniquement** par un JSON valide, sans texte en dehors du JSON.\n",
    "\n",
    "Texte:\n",
    "La classification supervisée apprend une fonction qui associe des entrées à des étiquettes à partir d'exemples annotés.\n",
    "\"\"\"\n",
    "\n",
    "raw = client.chat(system=\"\", user=prompt_json, params=DEFAULT_PARAMS)\n",
    "print(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e07d17-89d4-4f77-8857-4ebbabc09181",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "422af63f-f51e-4524-a917-9fd887d50959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON valide. Titre: Classification supervisée\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"titre\": {\"type\": \"string\"},\n",
    "        \"mots_cles\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "        \"resume\": {\"type\": \"string\"}\n",
    "    },\n",
    "    \"required\": [\"titre\", \"mots_cles\", \"resume\"]\n",
    "}\n",
    "\n",
    "try:\n",
    "    data = json.loads(raw)\n",
    "    validate(data, schema)\n",
    "    print(\"JSON valide. Titre:\", data[\"titre\"])\n",
    "except (json.JSONDecodeError, ValidationError) as e:\n",
    "    print(\"Sortie invalide:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2c8c0-abb5-4ced-8aa8-c2f9543cf6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
