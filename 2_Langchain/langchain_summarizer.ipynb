{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ez9yOti0wAcw"
   },
   "source": [
    "# Build a News Articles Summarizer - Solution\n",
    "\n",
    "Welcome to the solution notebook! This contains completed implementations for building an AI-powered news article summarizer using LangChain.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Article extraction and parsing\n",
    "- Prompt template creation\n",
    "- Multi-backend LLM integration (OpenAI/Ollama)\n",
    "- Language and format customization\n",
    "\n",
    "## Solutions Included\n",
    "\n",
    "Each exercise is completed with best practices and production-ready code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZggZ0nYwX6P"
   },
   "source": [
    "### API Configuration\n",
    "\n",
    "This notebook supports both OpenAI API and local Ollama. Choose your backend:\n",
    "\n",
    "**Option 1: OpenAI**\n",
    "- Set `OPENAI_API_KEY` environment variable\n",
    "- More reliable, requires internet connection\n",
    "- Pay per use (check current pricing)\n",
    "\n",
    "**Option 2: Ollama (Local)**\n",
    "- Set `USE_OLLAMA=1` environment variable\n",
    "- Run `ollama serve` in a terminal\n",
    "- Free, requires local GPU or enough RAM\n",
    "- Install models with `ollama pull mistral`\n",
    "\n",
    "**Recommendation:** Start with OpenAI for simplicity, then try Ollama for local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_6rH09GpWyP_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Meta claims its new AI supercomputer will set records\n",
      "Text: Meta (formerly Facebook) has unveiled an AI supercomputer that it claims will be the world’s fastest.\n",
      "\n",
      "The supercomputer is called the AI Research SuperCluster (RSC) and is yet to be fully complete. However, Meta’s researchers have already begun using it for training large natural language processing (NLP) and computer vision models.\n",
      "\n",
      "RSC is set to be fully built in mid-2022. Meta says that it will be the fastest in the world once complete and the aim is for it to be capable of training models with trillions of parameters.\n",
      "\n",
      "“We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together,” wrote Meta in a blog post.\n",
      "\n",
      "“Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.”\n",
      "\n",
      "For production, Meta expects RSC will be 20x faster than Meta’s current V100-based clusters. RSC is also estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\n",
      "\n",
      "A model with tens of billions of parameters can finish training in three weeks compared with nine weeks prior to RSC.\n",
      "\n",
      "Meta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets. RSC was designed with the security and privacy controls in mind to allow Meta to use real-world examples from its production systems in production training.\n",
      "\n",
      "What this means in practice is that Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms—using real data from them.\n",
      "\n",
      "“We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale,” says Meta.\n",
      "\n",
      "(Image Credit: Meta)\n",
      "\n",
      "Want to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022.\n",
      "\n",
      "Explore other upcoming enterprise technology events and webinars powered by TechForge here.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'\n",
    "}\n",
    "\n",
    "article_url = \"https://www.artificialintelligence-news.com/2022/01/25/meta-claims-new-ai-supercomputer-will-set-records/\"\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "try:\n",
    "    response = session.get(article_url, headers=headers, timeout=10)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        print(f\"Title: {article.title}\")\n",
    "        print(f\"Text: {article.text}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch article at {article_url}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while fetching article at {article_url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYaDVYasynOI"
   },
   "source": [
    "## 2. Build a Prompt Template\n",
    "\n",
    "**Solution:** The template uses f-string formatting to inject the article title and text into the prompt.\n",
    "\n",
    "**Best practices shown:**\n",
    "- Clear role definition for the AI\n",
    "- Visual separators for context\n",
    "- Explicit task instructions\n",
    "- Using f-strings for dynamic content injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-neGI_O-WyH5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are a very good assistant that summarizes online articles.\\n\\nHere's the article you want to summarize.\\n\\n==================\\nTitle: Meta claims its new AI supercomputer will set records\\n\\nMeta (formerly Facebook) has unveiled an AI supercomputer that it claims will be the world’s fastest.\\n\\nThe supercomputer is called the AI Research SuperCluster (RSC) and is yet to be fully complete. However, Meta’s researchers have already begun using it for training large natural language processing (NLP) and computer vision models.\\n\\nRSC is set to be fully built in mid-2022. Meta says that it will be the fastest in the world once complete and the aim is for it to be capable of training models with trillions of parameters.\\n\\n“We hope RSC will help us build entirely new AI systems that can, for example, power real-time voice translations to large groups of people, each speaking a different language, so they can seamlessly collaborate on a research project or play an AR game together,” wrote Meta in a blog post.\\n\\n“Ultimately, the work done with RSC will pave the way toward building technologies for the next major computing platform — the metaverse, where AI-driven applications and products will play an important role.”\\n\\nFor production, Meta expects RSC will be 20x faster than Meta’s current V100-based clusters. RSC is also estimated to be 9x faster at running the NVIDIA Collective Communication Library (NCCL) and 3x faster at training large-scale NLP workflows.\\n\\nA model with tens of billions of parameters can finish training in three weeks compared with nine weeks prior to RSC.\\n\\nMeta says that its previous AI research infrastructure only leveraged open source and other publicly-available datasets. RSC was designed with the security and privacy controls in mind to allow Meta to use real-world examples from its production systems in production training.\\n\\nWhat this means in practice is that Meta can use RSC to advance research for vital tasks such as identifying harmful content on its platforms—using real data from them.\\n\\n“We believe this is the first time performance, reliability, security, and privacy have been tackled at such a scale,” says Meta.\\n\\n(Image Credit: Meta)\\n\\nWant to learn more about AI and big data from industry leaders? Check out AI & Big Data Expo. The next events in the series will be held in Santa Clara on 11-12 May 2022, Amsterdam on 20-21 September 2022, and London on 1-2 December 2022.\\n\\nExplore other upcoming enterprise technology events and webinars powered by TechForge here.\\n==================\\n\\nWrite a summary of the previous article.\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# prepare template for prompt\n",
    "template = f\"\"\"You are a very good assistant that summarizes online articles.\n",
    "\n",
    "Here's the article you want to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article.title}\n",
    "\n",
    "{article.text}\n",
    "==================\n",
    "\n",
    "Write a summary of the previous article.\n",
    "\"\"\"\n",
    "\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqXOXTY81rGz"
   },
   "source": [
    "## 3. Initialize Language Model\n",
    "\n",
    "**Solution:** The code initializes the appropriate LLM based on the `USE_OLLAMA` environment variable.\n",
    "\n",
    "**Key implementation details:**\n",
    "- Conditional logic to select backend\n",
    "- Environment variable detection\n",
    "- Proper model initialization with temperature=0\n",
    "- User feedback on selected backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "byEkwrXedq--"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Utilisation du backend Gemini (gemini-2.5-flash)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Determine which backend to use in the .env\n",
    "USE_OLLAMA = os.environ.get(\"USE_OLLAMA\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "USE_GEMINI = os.environ.get(\"USE_GEMINI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    # Utiliser Ollama\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "    # Assurez-vous que le serveur Ollama est en cours d'exécution\n",
    "    # et que le modèle 'mistral' est téléchargé.\n",
    "    llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "    print(\"⚙️ Utilisation du backend Ollama (mistral)\")\n",
    "elif USE_GEMINI:\n",
    "    # Utiliser Gemini\n",
    "    # Assurez-vous que la variable d'environnement GOOGLE_API_KEY est définie dans le .env\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    print(\"⚙️ Utilisation du backend Gemini (gemini-2.5-flash)\")\n",
    "else:\n",
    "    # Utiliser OpenAI par défaut\n",
    "    # Assurez-vous que la variable d'environnement OPENAI_API_KEY est définie dans le .env\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    print(\"⚙️ Utilisation du backend OpenAI (gpt-4o-mini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs4UPuXr18YQ"
   },
   "source": [
    "## 4. Generate a Summary\n",
    "\n",
    "**Solution:** Using `.invoke()` method with messages list.\n",
    "\n",
    "**How it works:**\n",
    "- Pass the list of messages to the LLM\n",
    "- The model processes the prompt and returns a response\n",
    "- Access content via `.content` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hVMxNufYXMek"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Meta has announced its new AI supercomputer, the AI Research SuperCluster (RSC), which it claims will be the world's fastest upon its full completion in mid-2022. Researchers are already using the RSC to train large natural language processing (NLP) and computer vision models, with the goal of training models with trillions of parameters.\\n\\nMeta expects RSC to be significantly faster than its current systems, boasting 20x faster production, 9x faster NVIDIA Collective Communication Library (NCCL) performance, and 3x faster large-scale NLP training. For instance, a model that previously took nine weeks to train can now finish in three weeks.\\n\\nThe company intends to use RSC to develop new AI systems for applications like real-time voice translations and to build technologies for the metaverse. A key feature of RSC is its enhanced security and privacy controls, which will allow Meta to use real-world data from its production systems to train models, enabling advancements in tasks such as identifying harmful content on its platforms. Meta highlights this as a first for tackling performance, reliability, security, and privacy at such a scale.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = llm.invoke(messages)\n",
    "summary.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAmlZKfJ4RLq"
   },
   "source": [
    "## 5. Advanced: Customized Output Format\n",
    "\n",
    "**Solution:** Creating a French bullet-point summary with detailed prompt engineering.\n",
    "\n",
    "**This solution demonstrates:**\n",
    "- Advanced prompt construction with `.format()` method\n",
    "- Multi-language output (French)\n",
    "- Specific formatting requirements (bullets)\n",
    "- Reusable template pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PPwrM1PWXtBr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voici un résumé de l'article en français :\n",
      "\n",
      "*   Meta (anciennement Facebook) a dévoilé un superordinateur d'IA nommé AI Research SuperCluster (RSC).\n",
      "*   Meta affirme que le RSC sera le plus rapide du monde une fois entièrement construit, ce qui est prévu pour la mi-2022.\n",
      "*   Il est déjà utilisé pour l'entraînement de grands modèles de traitement du langage naturel (NLP) et de vision par ordinateur.\n",
      "*   L'objectif est qu'il puisse entraîner des modèles avec des milliers de milliards de paramètres.\n",
      "*   Le RSC vise à développer de nouveaux systèmes d'IA pour des applications telles que la traduction vocale en temps réel et les technologies du métavers.\n",
      "*   Meta s'attend à ce qu'il soit 20 fois plus rapide que ses clusters actuels basés sur V100 et 3 fois plus rapide pour les flux de travail NLP à grande échelle.\n",
      "*   Grâce au RSC, l'entraînement d'un modèle de plusieurs dizaines de milliards de paramètres prendra trois semaines, contre neuf auparavant.\n",
      "*   Conçu avec des contrôles de sécurité et de confidentialité, le RSC permettra d'utiliser des exemples réels des systèmes de production de Meta pour l'entraînement, notamment pour identifier les contenus nuisibles.\n",
      "*   Meta souligne que c'est la première fois que la performance, la fiabilité, la sécurité et la confidentialité sont abordées à une telle échelle.\n"
     ]
    }
   ],
   "source": [
    "# prepare template for prompt\n",
    "template = \"\"\"You are an advanced AI assistant that summarizes online articles into bulleted lists in French.\n",
    "\n",
    "Here's the article you need to summarize.\n",
    "\n",
    "==================\n",
    "Title: {article_title}\n",
    "\n",
    "{article_text}\n",
    "==================\n",
    "\n",
    "Now, provide a summarized version of the article in a bulleted list format, in French.\n",
    "\"\"\"\n",
    "\n",
    "# format prompt\n",
    "prompt = template.format(article_title=article.title, article_text=article.text)\n",
    "\n",
    "# generate summary\n",
    "summary = llm.invoke([HumanMessage(content=prompt)])\n",
    "print(summary.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
