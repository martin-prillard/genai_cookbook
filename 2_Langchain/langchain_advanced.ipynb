{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236d2e9d",
   "metadata": {},
   "source": [
    "# LangChain — Messages, Memory, Structured Output & Guardrailing (Solution)\n",
    "\n",
    "Welcome to the solution notebook! This contains complete, production-ready implementations for all exercises.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Complete solutions for five critical LangChain concepts:\n",
    "1. **Messages**: Typed message classes and prompt templates with LCEL\n",
    "2. **Memory**: Conversation history and user profiles\n",
    "3. **Structured Output**: Type-safe outputs with Pydantic\n",
    "4. **Guardrails**: Production safety patterns\n",
    "\n",
    "## Best Practices Shown\n",
    "\n",
    "Each solution demonstrates:\n",
    "- Clean, readable code\n",
    "- Production-ready patterns\n",
    "- Proper error handling\n",
    "- Performance considerations\n",
    "\n",
    "> **Note**: This notebook supports both OpenAI API and local Ollama. Set `USE_OLLAMA=1` to use Ollama, otherwise OpenAI will be used.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f62622",
   "metadata": {},
   "source": [
    "## 1. LangChain Messages\n",
    "\n",
    "**Solution:** Using typed messages with proper structure.\n",
    "\n",
    "**Implementation details:**\n",
    "- `SystemMessage`: Sets the AI's role and behavior\n",
    "- `HumanMessage`: User input with clear typing\n",
    "- Response access via `.content` attribute\n",
    "- `.type` for message type introspection\n",
    "\n",
    "**Best practices:**\n",
    "- Always define system role for consistency\n",
    "- Use typed messages instead of raw strings\n",
    "- Check message type when building complex flows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3149e0d4-46f9-46ec-a9b7-12a45407b09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Utilisation du backend OpenAI (gpt-4o-mini)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Determine which backend to use in the .env\n",
    "USE_OLLAMA = os.environ.get(\"USE_OLLAMA\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "USE_GEMINI = os.environ.get(\"USE_GEMINI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    # Utiliser Ollama\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "    # Assurez-vous que le serveur Ollama est en cours d'exécution\n",
    "    # et que le modèle 'mistral' est téléchargé.\n",
    "    llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "    print(\"⚙️ Utilisation du backend Ollama (mistral)\")\n",
    "elif USE_GEMINI:\n",
    "    # Utiliser Gemini\n",
    "    # Assurez-vous que la variable d'environnement GOOGLE_API_KEY est définie dans le .env\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    print(\"⚙️ Utilisation du backend Gemini (gemini-2.5-flash)\")\n",
    "else:\n",
    "    # Utiliser OpenAI par défaut\n",
    "    # Assurez-vous que la variable d'environnement OPENAI_API_KEY est définie dans le .env\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    print(\"⚙️ Utilisation du backend OpenAI (gpt-4o-mini)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99280b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai -> Embeddings are numerical representations of objects, such as words or sentences, in a continuous vector space, allowing for the capture of semantic relationships and similarities between them. They enable machine learning models to process and understand complex data by transforming categorical or high-dimensional inputs into lower-dimensional, dense vectors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a very bad tutor giving fake information.\"),\n",
    "    HumanMessage(content=\"Explain embeddings in two sentences.\"),\n",
    "]\n",
    "resp = llm.invoke(messages)\n",
    "print(resp.type, \"->\", resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49561278",
   "metadata": {},
   "source": [
    "### 1.1 Messages via `ChatPromptTemplate` + LCEL\n",
    "\n",
    "**Solution:** LCEL chain composition with pipe operator.\n",
    "\n",
    "**Chain components:**\n",
    "1. `ChatPromptTemplate`: Dynamic prompt with `{topic}` variable\n",
    "2. `llm`: Language model invocation\n",
    "3. `StrOutputParser`: Converts message objects to plain strings\n",
    "\n",
    "**Why this pattern?**\n",
    "- Separation of concerns (prompt, model, parser)\n",
    "- Easy to add steps (filters, validators, etc.)\n",
    "- Reusable across different use cases\n",
    "- Supports streaming and batching out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2299631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- **Definition**: Tokenization is the process of converting sensitive data, such as credit card numbers or personal information, into unique identification symbols (tokens) that retain essential information without compromising security.\\n\\n- **Purpose**: The primary goal of tokenization is to protect sensitive data from unauthorized access and breaches, thereby reducing the risk of fraud and ensuring compliance with data protection regulations.\\n\\n- **Implementation**: Tokenization can be applied in various contexts, including payment processing, data storage, and cloud computing, often involving a secure tokenization system that maps tokens back to the original data only in a controlled and secure environment.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a precise technical writer.\"),\n",
    "    (\"user\", \"Summarize {topic} in 3 bullet points.\")\n",
    "])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"topic\":\"tokenization\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33a853",
   "metadata": {},
   "source": [
    "## 2. Short-term Memory (Conversation History)\n",
    "\n",
    "**Solution:** Stateful conversation management with session tracking.\n",
    "\n",
    "**Key components:**\n",
    "- `InMemoryChatMessageHistory`: Stores messages per session\n",
    "- `get_history()`: Factory function for session management\n",
    "- `MessagesPlaceholder`: Injects history into prompts\n",
    "- `RunnableWithMessageHistory`: Wraps chain with memory\n",
    "\n",
    "**Pattern explanation:**\n",
    "- Session ID maps to chat history store\n",
    "- History automatically loaded and injected\n",
    "- Thread-safe for multi-user applications\n",
    "- Can be extended to persistent storage (Redis, DB)\n",
    "\n",
    "**Production tip:** For production, replace `store = {}` with Redis or a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c466413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: Hello! I’m an AI assistant here to help you with information, answer questions, and provide support on a variety of topics. How can I assist you today?\n",
      "Turn 2: You asked, \"Hello, who are you?\" If you have any more questions or need assistance with something specific, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "store = {}\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain2 = prompt2 | llm | StrOutputParser()\n",
    "\n",
    "history_chain = RunnableWithMessageHistory(\n",
    "    chain2, get_history, input_messages_key=\"input\", history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "cfg = {\"configurable\": {\"session_id\": \"thread-1\"}}\n",
    "print(\"Turn 1:\", history_chain.invoke({\"input\":\"Hello, who are you?\"}, cfg)[:200])\n",
    "print(\"Turn 2:\", history_chain.invoke({\"input\":\"What did I just ask you?\"}, cfg)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996eebf6",
   "metadata": {},
   "source": [
    "## 3. Long-term Memory (User Profiles)\n",
    "\n",
    "**Solution:** JSON-based persistent user knowledge storage.\n",
    "\n",
    "**Implementation breakdown:**\n",
    "- `load_ltm()`: Safely loads JSON file, returns empty dict if missing\n",
    "- `save_ltm()`: Creates directory structure if needed\n",
    "- `render_profile()`: Formats profile as readable text\n",
    "- Profile injection: Added to system message\n",
    "\n",
    "**Storage pattern:**\n",
    "- Simple JSON for structured data\n",
    "- No need for vectors (unlike semantic search)\n",
    "- Persists across sessions\n",
    "- Easy to query and update\n",
    "\n",
    "**When to use:**\n",
    "- User preferences, names, roles\n",
    "- Application-wide settings\n",
    "- Constraint information\n",
    "- For semantic similarity, use vector stores instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18159c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are Alex, a student.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os, pathlib\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "MEM_PATH = \"data/ltm_store.json\"\n",
    "def load_ltm():\n",
    "    if os.path.exists(MEM_PATH):\n",
    "        return json.load(open(MEM_PATH))\n",
    "    return {}\n",
    "\n",
    "def save_ltm(d):\n",
    "    pathlib.Path(MEM_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "    json.dump(d, open(MEM_PATH,\"w\"))\n",
    "\n",
    "ltm = load_ltm()\n",
    "ltm.setdefault(\"user_profile\", {\"name\":\"Alex\", \"role\":\"Student\", \"prefers\":\"concise answers\"})\n",
    "save_ltm(ltm)\n",
    "\n",
    "def render_profile(d): return \"\\n\".join(f\"- {k}: {v}\" for k,v in d.items())\n",
    "    \n",
    "profile = render_profile(ltm[\"user_profile\"])\n",
    "\n",
    "profile_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Use the following long-term profile to personalize replies:\\n{profile}\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "profile_chain = profile_prompt | llm | StrOutputParser()\n",
    "profile_chain.invoke({\"profile\": profile, \"question\": \"Who am I?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7919fa4",
   "metadata": {},
   "source": [
    "## 4. Structured Output with Pydantic\n",
    "\n",
    "**Solution:** Type-safe output generation using Pydantic models.\n",
    "\n",
    "**Model definition:**\n",
    "- `BaseModel`: Foundation for data models\n",
    "- `Field()`: Rich field descriptions and constraints\n",
    "- `ge=1, le=240`: Numeric bounds validation\n",
    "- `List[str]`: Type-hinted collections\n",
    "\n",
    "**Benefits:**\n",
    "- Automatic validation (guaranteed structure)\n",
    "- IDE autocomplete support\n",
    "- Direct JSON serialization\n",
    "- Field-level error messages\n",
    "- Natural integration with databases/APIs\n",
    "\n",
    "**Production pattern:** Use structured outputs for all predictable data shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4c23e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StudyPlan(topic='Tokenization', steps=['Read introductory articles on tokenization to understand the basic concepts and applications.', 'Watch video tutorials that explain tokenization in natural language processing and blockchain.', 'Practice coding tokenization using libraries like NLTK or SpaCy for NLP, and explore token standards like ERC-20 for blockchain.', 'Engage in online forums or study groups to discuss tokenization concepts and share insights with peers.', 'Complete a small project that involves implementing tokenization in a real-world scenario, such as text analysis or creating a simple token on a blockchain.'], estimated_minutes=240, urls=['https://www.ibm.com/cloud/learn/tokenization', 'https://towardsdatascience.com/tokenization-in-nlp-101-5c1c1c1c1c1c', 'https://www.analyticsvidhya.com/blog/2021/06/a-beginners-guide-to-tokenization-in-nlp/', 'https://www.blockchain-council.org/blockchain/what-is-tokenization-in-blockchain/', 'https://www.datacamp.com/community/tutorials/tokenization-nlp-python'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class StudyPlan(BaseModel):\n",
    "    topic: str = Field(..., description=\"Main topic\")\n",
    "    steps: List[str] = Field(..., description=\"3-5 actionable steps\")\n",
    "    estimated_minutes: int = Field(..., ge=1, le=240)\n",
    "    urls: List[str] =  Field(..., description=\"true urls to learn the topic\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(StudyPlan)\n",
    "structured_llm.invoke(\"Create a study plan to learn tokenization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655484bb",
   "metadata": {},
   "source": [
    "## 5. Guardrails: Production Safety\n",
    "\n",
    "**Solution:** Multi-layer safety system for production applications.\n",
    "\n",
    "**Implementation:**\n",
    "\n",
    "**Pre-guard (input filtering):**\n",
    "- Regex pattern matching with `DISALLOWED`\n",
    "- Raises exceptions before API call\n",
    "- Prevents prompt injection\n",
    "- Fast rejection of unsafe content\n",
    "\n",
    "**Post-guard (output validation):**\n",
    "- Length limits to prevent abuse\n",
    "- Truncation with ellipsis\n",
    "- Additional regex checks possible\n",
    "- Sanitization layer\n",
    "\n",
    "**Production considerations:**\n",
    "- Combine with Pydantic validation (layer 2)\n",
    "- Log blocked attempts for monitoring\n",
    "- Consider ML-based content moderation\n",
    "- Add rate limiting separately\n",
    "- Use permissive deny lists\n",
    "\n",
    "**Testing tip:** Try different malicious inputs to test your guards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7fd11be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe query: Machine learning is a subset of artificial intelligence that involves the development of algorithms and statistical models that enable computers to learn from and make predictions or decisions based on data. It allows systems to improve their performance on a specific task over time without being explicitly programmed for each scenario....\n",
      "\n",
      "Unsafe query: I'm sorry, but I cannot respond to that request due to security policies.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "DISALLOWED = re.compile(r\"(?i)credit card|ssn|social security|violent threat\")\n",
    "\n",
    "def pre_guard(user_input: str):\n",
    "    if DISALLOWED.search(user_input):\n",
    "        raise ValueError(\"Input blocked by policy.\")\n",
    "\n",
    "def post_guard(text: str):\n",
    "    if len(text) > 100:\n",
    "        return text[:1000] + \"...\"\n",
    "    return text\n",
    "\n",
    "guard_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Answer briefly and avoid sensitive data.\"),\n",
    "    (\"user\",\"{q}\")\n",
    "])\n",
    "guard_chain = guard_prompt | llm | StrOutputParser()\n",
    "\n",
    "def safe_respond(q: str):\n",
    "    try:\n",
    "        pre_guard(q)\n",
    "        return post_guard(guard_chain.invoke({\"q\": q}))\n",
    "    except ValueError as e:\n",
    "        # Return a friendly message instead of raising an error\n",
    "        return \"I'm sorry, but I cannot respond to that request due to security policies.\"\n",
    "\n",
    "# Test with a safe query\n",
    "print(\"Safe query:\", safe_respond(\"What is machine learning?\"))\n",
    "\n",
    "# Test with an unsafe query (should return the friendly error message instead of crashing)\n",
    "print(\"\\nUnsafe query:\", safe_respond(\"Give me your ssn.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3367e",
   "metadata": {},
   "source": [
    "## 6. Combining Concepts: Safe Structured Output\n",
    "\n",
    "**Solution:** Production-ready chain combining structured output with guardrails.\n",
    "\n",
    "**Implementation strategy:**\n",
    "- Reuse `pre_guard` from exercise 5 for input validation\n",
    "- Define a Pydantic model for technology information\n",
    "- Use `with_structured_output()` to get typed responses\n",
    "- Handle validation errors gracefully\n",
    "- Return user-friendly messages on failure\n",
    "\n",
    "**Key patterns:**\n",
    "- Layered validation (input → processing → output)\n",
    "- Graceful error handling for production\n",
    "- Structured data extraction with type safety\n",
    "- Safe error messages (no sensitive info leaked)\n",
    "\n",
    "**Production considerations:**\n",
    "- Log validation failures for monitoring\n",
    "- Consider rate limiting\n",
    "- Add output sanitization if needed\n",
    "- Use for API endpoints that need structured responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d468e70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - Safe query:\n",
      "<class '__main__.TechInfo'>\n",
      "name='Transformers' category='NLP' description='Transformers are a type of neural network architecture that has revolutionized natural language processing (NLP) by enabling models to process sequences of data in parallel, rather than sequentially. They utilize mechanisms called attention to weigh the importance of different words in a sentence, allowing for better context understanding and improved performance on various NLP tasks such as translation, summarization, and text generation.' difficulty='intermediate'\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 2 - Unsafe query (should be blocked):\n",
      "I'm sorry, but I cannot respond to that request due to security policies.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# Define Pydantic model for technology information\n",
    "class TechInfo(BaseModel):\n",
    "    name: str = Field(..., description=\"Technology name\")\n",
    "    category: str = Field(..., description=\"Technology category (e.g., ML, NLP, Computer Vision)\")\n",
    "    description: str = Field(..., description=\"Brief description of the technology\")\n",
    "    difficulty: Optional[str] = Field(None, description=\"Difficulty level (beginner, intermediate, advanced)\")\n",
    "\n",
    "# Create structured LLM\n",
    "structured_llm = llm.with_structured_output(TechInfo)\n",
    "\n",
    "def safe_extract_tech_info(query: str):\n",
    "    \"\"\"\n",
    "    Safely extract technology information with input validation and error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Pre-guard validation\n",
    "        pre_guard(query)\n",
    "        \n",
    "        # Step 2: Extract structured information\n",
    "        result = structured_llm.invoke(f\"Extract information about this technology: {query}\")\n",
    "        print(type(result))\n",
    "\n",
    "        # Step 3: Post-guard validation (optional: check if result is complete)\n",
    "        # Pydantic already validates required fields, but we can add custom checks\n",
    "        if not result.name or not result.description:\n",
    "            return \"I couldn't extract complete information. Please try rephrasing your query.\"\n",
    "        \n",
    "        return result\n",
    "    except ValueError:\n",
    "        # Input was blocked by pre-guard\n",
    "        return \"I'm sorry, but I cannot respond to that request due to security policies.\"\n",
    "    except Exception as e:\n",
    "        # Handle other errors (API failures, parsing errors, etc.)\n",
    "        return f\"I encountered an error processing your request. Please try again.\"\n",
    "\n",
    "# Test cases\n",
    "print(\"Test 1 - Safe query:\")\n",
    "result1 = safe_extract_tech_info(\"Tell me about transformers\")\n",
    "print(result1)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Test 2 - Unsafe query (should be blocked):\")\n",
    "result2 = safe_extract_tech_info(\"Give me your ssn\")\n",
    "print(result2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caffff6d",
   "metadata": {},
   "source": [
    "## 7. Advanced LCEL: Multi-Chain Routing\n",
    "\n",
    "**Solution:** Intelligent routing system that selects chains based on user intent.\n",
    "\n",
    "**Architecture:**\n",
    "- **Technical Chain**: Structured output for technical questions\n",
    "- **Conversational Chain**: Memory-enabled chat for general conversation\n",
    "- **Safety Handler**: Blocks unsafe requests\n",
    "\n",
    "**Routing logic:**\n",
    "1. Check for unsafe content (highest priority)\n",
    "2. Detect technical keywords (explain, what is, how does, etc.)\n",
    "3. Default to conversational chain\n",
    "\n",
    "**Implementation patterns:**\n",
    "- Intent detection using keyword matching (production: use classifier)\n",
    "- Session management for conversational chain\n",
    "- Structured output for technical queries\n",
    "- Graceful error handling throughout\n",
    "\n",
    "**Production enhancements:**\n",
    "- Use ML classifier for intent detection\n",
    "- Add fallback chain for unrecognized intents\n",
    "- Log routing decisions for analytics\n",
    "- Cache technical responses when possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261f7554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 - Technical query:\n",
      "Topic: Transformer\n",
      "\n",
      "Explanation: A transformer is an electrical device that transfers electrical energy between two or more circuits through electromagnetic induction. It is primarily used to increase (step-up) or decrease (step-down) voltage levels in alternating current (AC) systems. Transformers consist of two or more wire coils (windings) wrapped around a magnetic core. When an alternating current flows through one coil (the primary winding), it creates a magnetic field that induces a voltage in the other coil (the secondary winding). The ratio of the number of turns in the primary and secondary coils determines the voltage change.\n",
      "\n",
      "Key Points:\n",
      "  • Transformers operate on the principle of electromagnetic induction.\n",
      "  • They can step up or step down voltage levels in AC systems.\n",
      "  • The voltage ratio is determined by the turns ratio of the coils.\n",
      "  • Transformers are essential for efficient power transmission over long distances.\n",
      "  • They are used in various applications, including power distribution, audio equipment, and electronic devices.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 2 - Conversational query:\n",
      "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 3 - Follow-up (should use memory):\n",
      "You asked, \"How are you doing?\" If you have any other questions or need assistance, feel free to let me know!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test 4 - Unsafe query (should be blocked):\n",
      "I'm sorry, but I cannot respond to that request due to security policies.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ========== 1. Technical Chain (Structured Output) ==========\n",
    "class TechnicalAnswer(BaseModel):\n",
    "    topic: str = Field(..., description=\"The topic being explained\")\n",
    "    explanation: str = Field(..., description=\"Clear explanation of the topic\")\n",
    "    key_points: List[str] = Field(..., description=\"3-5 key points about the topic\")\n",
    "\n",
    "technical_llm = llm.with_structured_output(TechnicalAnswer)\n",
    "\n",
    "technical_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a technical expert. Provide clear, structured explanations.\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "\n",
    "technical_chain = technical_prompt | technical_llm\n",
    "\n",
    "# ========== 2. Conversational Chain (With Memory) ==========\n",
    "conversational_store = {}\n",
    "\n",
    "def get_conversational_history(session_id: str):\n",
    "    if session_id not in conversational_store:\n",
    "        conversational_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return conversational_store[session_id]\n",
    "\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly and helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])\n",
    "\n",
    "conversational_base = conversational_prompt | llm | StrOutputParser()\n",
    "\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    conversational_base,\n",
    "    get_conversational_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# ========== 3. Safety Handler ==========\n",
    "# Reuse safe_respond from exercise 5\n",
    "\n",
    "# ========== Router Function ==========\n",
    "def smart_router(query: str, session_id: str = \"default\"):\n",
    "    \"\"\"\n",
    "    Routes queries to appropriate chain based on intent detection.\n",
    "    Priority: Safety > Technical > Conversational\n",
    "    \"\"\"\n",
    "    # Step 1: Check for unsafe content (highest priority)\n",
    "    try:\n",
    "        pre_guard(query)\n",
    "    except ValueError:\n",
    "        return \"I'm sorry, but I cannot respond to that request due to security policies.\"\n",
    "    \n",
    "    # Step 2: Detect technical intent\n",
    "    technical_keywords = [\"explain\", \"what is\", \"how does\", \"define\", \"describe\", \"tell me about\"]\n",
    "    query_lower = query.lower()\n",
    "    is_technical = any(keyword in query_lower for keyword in technical_keywords)\n",
    "    \n",
    "    # Step 3: Route to appropriate chain\n",
    "    if is_technical:\n",
    "        # Use technical chain with structured output\n",
    "        try:\n",
    "            cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "            result = technical_chain.invoke({\"query\": query})\n",
    "            # Format structured output for display\n",
    "            key_points_str = \"\\n\".join(f\"  • {point}\" for point in result.key_points)\n",
    "            return f\"Topic: {result.topic}\\n\\nExplanation: {result.explanation}\\n\\nKey Points:\\n{key_points_str}\"\n",
    "        except Exception as e:\n",
    "            return f\"I encountered an error processing your technical question. Please try again.\"\n",
    "    else:\n",
    "        # Use conversational chain with memory\n",
    "        try:\n",
    "            cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "            response = conversational_chain.invoke({\"query\": query}, cfg)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"I encountered an error processing your message. Please try again.\"\n",
    "\n",
    "# ========== Test Cases ==========\n",
    "print(\"Test 1 - Technical query:\")\n",
    "print(smart_router(\"What is a transformer?\", \"session-1\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Test 2 - Conversational query:\")\n",
    "print(smart_router(\"How are you doing?\", \"session-1\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Test 3 - Follow-up (should use memory):\")\n",
    "print(smart_router(\"What did I just ask?\", \"session-1\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Test 4 - Unsafe query (should be blocked):\")\n",
    "print(smart_router(\"Give me your credit card\", \"session-1\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc5430",
   "metadata": {},
   "source": [
    "## 8. Improved Routing: LLM-Based Intent Classification\n",
    "\n",
    "**Improvement:** Replace keyword matching with an LLM classifier for more robust intent detection.\n",
    "\n",
    "**Why this is better:**\n",
    "- **Context-aware**: Understands intent beyond simple keyword matching\n",
    "- **Handles variations**: Works with paraphrased queries and different phrasings\n",
    "- **More accurate**: Can distinguish nuanced differences (e.g., \"tell me a story\" vs \"tell me about transformers\")\n",
    "- **Extensible**: Easy to add new intent categories without hardcoding keywords\n",
    "\n",
    "**Implementation approach:**\n",
    "- Use structured output with Pydantic to get typed classification\n",
    "- Create a dedicated classification chain that runs before routing\n",
    "- Maintain the same safety and routing logic, but with better intent detection\n",
    "\n",
    "**Production benefits:**\n",
    "- Better user experience (fewer misrouted queries)\n",
    "- Can handle edge cases that keyword matching misses\n",
    "- Easy to fine-tune by adjusting the classification prompt\n",
    "- Can be extended to multi-class classification (technical, conversational, support, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60205ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED ROUTER WITH LLM CLASSIFICATION\n",
      "============================================================\n",
      "\n",
      "Test 1 - Technical query (clear):\n",
      "Query: 'What is a transformer?'\n",
      "Topic: Transformer\n",
      "\n",
      "Explanation: A transformer is an electrical device that transfers electrical energy between two or more circuits through electromagnetic induction. It is primarily used to increase (step-up) or decrease (step-down) voltage levels in power systems, allowing for efficient transmiss...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Test 2 - Technical query (ambiguous - keyword matching might fail):\n",
      "Query: 'I need to understand how neural networks process sequences'\n",
      "Topic: Neural Networks and Sequence Processing\n",
      "\n",
      "Explanation: Neural networks process sequences using specialized architectures designed to handle data where the order of elements is significant. The most common types of neural networks for sequence processing are Recurrent Neural Networks (RNNs) and...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Test 3 - Conversational query:\n",
      "Query: 'How are you doing today?'\n",
      "I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\n",
      "\n",
      "============================================================\n",
      "\n",
      "Test 4 - Ambiguous query (context-dependent):\n",
      "Query: 'Tell me about transformers'\n",
      "(LLM should classify based on context - likely technical in this educational setting)\n",
      "Topic: Transformers\n",
      "\n",
      "Explanation: Transformers are electrical devices used to transfer electrical energy between two or more circuits through electromagnetic induction. They are essential components in power distribution systems, allowing for the efficient transmission of electricity over long dista...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Test 5 - Follow-up (should use memory):\n",
      "Query: 'What did I just ask about?'\n",
      "You asked how I was doing today. If there's anything specific you'd like to know or discuss, feel free to let me know!\n",
      "\n",
      "============================================================\n",
      "\n",
      "Test 6 - Unsafe query (should be blocked):\n",
      "Query: 'Give me your credit card number'\n",
      "I'm sorry, but I cannot respond to that request due to security policies.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ========== 1. Define Intent Classification Model ==========\n",
    "class IntentType(str, Enum):\n",
    "    TECHNICAL = \"technical\"\n",
    "    CONVERSATIONAL = \"conversational\"\n",
    "\n",
    "class IntentClassification(BaseModel):\n",
    "    intent: IntentType = Field(..., description=\"The detected intent type\")\n",
    "    confidence: str = Field(..., description=\"Brief explanation of why this intent was chosen\")\n",
    "    \n",
    "# ========== 2. Create Classification Chain ==========\n",
    "classification_llm = llm.with_structured_output(IntentClassification)\n",
    "\n",
    "classification_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an intent classifier. Analyze user queries and determine if they are:\n",
    "- TECHNICAL: Questions asking for explanations, definitions, or technical information (e.g., \"What is X?\", \"Explain Y\", \"How does Z work?\")\n",
    "- CONVERSATIONAL: General chat, greetings, casual questions, or non-technical interactions (e.g., \"How are you?\", \"Tell me a joke\", \"What's the weather?\")\n",
    "\n",
    "Be context-aware: \"Tell me about transformers\" could be technical (if about AI) or conversational (if about movies).\n",
    "Use the context to make the best classification.\"\"\"),\n",
    "    (\"user\", \"Classify this query: {query}\")\n",
    "])\n",
    "\n",
    "classification_chain = classification_prompt | classification_llm\n",
    "\n",
    "# ========== 3. Improved Router with LLM Classification ==========\n",
    "def improved_smart_router(query: str, session_id: str = \"default\"):\n",
    "    \"\"\"\n",
    "    Improved router using LLM-based intent classification instead of keyword matching.\n",
    "    Priority: Safety > LLM Classification > Route to appropriate chain\n",
    "    \"\"\"\n",
    "    # Step 1: Check for unsafe content (highest priority)\n",
    "    try:\n",
    "        pre_guard(query)\n",
    "    except ValueError:\n",
    "        return \"I'm sorry, but I cannot respond to that request due to security policies.\"\n",
    "    \n",
    "    # Step 2: Use LLM to classify intent\n",
    "    try:\n",
    "        classification = classification_chain.invoke({\"query\": query})\n",
    "        intent = classification.intent\n",
    "        \n",
    "        # Step 3: Route based on LLM classification\n",
    "        if intent == IntentType.TECHNICAL:\n",
    "            # Use technical chain with structured output\n",
    "            try:\n",
    "                result = technical_chain.invoke({\"query\": query})\n",
    "                # Format structured output for display\n",
    "                key_points_str = \"\\n\".join(f\"  • {point}\" for point in result.key_points)\n",
    "                return f\"Topic: {result.topic}\\n\\nExplanation: {result.explanation}\\n\\nKey Points:\\n{key_points_str}\"\n",
    "            except Exception as e:\n",
    "                return f\"I encountered an error processing your technical question. Please try again.\"\n",
    "        else:  # CONVERSATIONAL\n",
    "            # Use conversational chain with memory\n",
    "            try:\n",
    "                cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "                response = conversational_chain.invoke({\"query\": query}, cfg)\n",
    "                return response\n",
    "            except Exception as e:\n",
    "                return f\"I encountered an error processing your message. Please try again.\"\n",
    "    except Exception as e:\n",
    "        # Fallback to conversational if classification fails\n",
    "        print(f\"Classification error: {e}, falling back to conversational\")\n",
    "        try:\n",
    "            cfg = {\"configurable\": {\"session_id\": session_id}}\n",
    "            response = conversational_chain.invoke({\"query\": query}, cfg)\n",
    "            return response\n",
    "        except Exception as e2:\n",
    "            return f\"I encountered an error processing your request. Please try again.\"\n",
    "\n",
    "# ========== Test Cases: Comparing Keyword vs LLM Classification ==========\n",
    "print(\"=\"*60)\n",
    "print(\"IMPROVED ROUTER WITH LLM CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTest 1 - Technical query (clear):\")\n",
    "print(\"Query: 'What is a transformer?'\")\n",
    "result = improved_smart_router(\"What is a transformer?\", \"session-llm-1\")\n",
    "print(result[:300] + \"...\" if len(result) > 300 else result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTest 2 - Technical query (ambiguous - keyword matching might fail):\")\n",
    "print(\"Query: 'I need to understand how neural networks process sequences'\")\n",
    "result = improved_smart_router(\"I need to understand how neural networks process sequences\", \"session-llm-1\")\n",
    "print(result[:300] + \"...\" if len(result) > 300 else result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTest 3 - Conversational query:\")\n",
    "print(\"Query: 'How are you doing today?'\")\n",
    "result = improved_smart_router(\"How are you doing today?\", \"session-llm-1\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTest 4 - Ambiguous query (context-dependent):\")\n",
    "print(\"Query: 'Tell me about transformers'\")\n",
    "print(\"(LLM should classify based on context - likely technical in this educational setting)\")\n",
    "result = improved_smart_router(\"Tell me about transformers\", \"session-llm-1\")\n",
    "print(result[:300] + \"...\" if len(result) > 300 else result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTest 5 - Follow-up (should use memory):\")\n",
    "print(\"Query: 'What did I just ask about?'\")\n",
    "result = improved_smart_router(\"What did I just ask about?\", \"session-llm-1\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTest 6 - Unsafe query (should be blocked):\")\n",
    "print(\"Query: 'Give me your credit card number'\")\n",
    "result = improved_smart_router(\"Give me your credit card number\", \"session-llm-1\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5c86f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
