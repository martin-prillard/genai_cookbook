{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent2Agent (A2A) Protocol: Build Interoperable Agents\n",
    "\n",
    "In this notebook, you'll learn the Agent2Agent (A2A) protocol basics and build two simple interoperable agents that can exchange structured messages. We'll:\n",
    "\n",
    "- Understand the motivation behind A2A and a minimal message schema\n",
    "- Configure an LLM model (OpenAI, Gemini, or Ollama) and a base `Agent` that speaks A2A\n",
    "- Implement two example agents (Researcher and Writer) and a simple mediator\n",
    "- Run a multi-turn exchange end-to-end\n",
    "- Practice with an exercise and a bonus tool-use pattern\n",
    "\n",
    "Prerequisites: \n",
    "- For OpenAI: environment variable `OPENAI_API_KEY` set with a valid key\n",
    "- For Gemini: environment variable `GOOGLE_API_KEY` set with a valid key\n",
    "- For Ollama: Ollama server running with the model installed\n",
    "- Set `USE_GEMINI=1` or `USE_OLLAMA=1` in your `.env` file to use those backends (defaults to OpenAI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility fix for langchain_google_genai with newer langchain_core\n",
    "# Run this cell FIRST before importing langchain_google_genai\n",
    "import sys\n",
    "import pydantic\n",
    "import warnings\n",
    "\n",
    "# Create a compatibility shim for pydantic_v1\n",
    "# This provides Pydantic v1 compatibility for langchain_google_genai\n",
    "class PydanticV1Compat:\n",
    "    \"\"\"Compatibility shim for pydantic_v1 imports\"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        # Handle root_validator specially for Pydantic v2 compatibility\n",
    "        if name == 'root_validator':\n",
    "            # Return a decorator that works with Pydantic v2\n",
    "            def root_validator(*args, **kwargs):\n",
    "                # Suppress warnings for deprecated root_validator\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    # Try to use model_validator if available (Pydantic v2)\n",
    "                    if hasattr(pydantic, 'model_validator'):\n",
    "                        return pydantic.model_validator(mode='before', *args, **kwargs)\n",
    "                    # Fallback to field_validator or other v2 validators\n",
    "                    return lambda f: f\n",
    "            return root_validator\n",
    "        return getattr(pydantic, name)\n",
    "\n",
    "# Patch langchain_core.pydantic_v1 if it doesn't exist\n",
    "try:\n",
    "    import langchain_core\n",
    "    if not hasattr(langchain_core, 'pydantic_v1'):\n",
    "        langchain_core.pydantic_v1 = PydanticV1Compat()\n",
    "        sys.modules['langchain_core.pydantic_v1'] = langchain_core.pydantic_v1\n",
    "        print(\"✅ Compatibility fix applied for langchain_google_genai\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Could not apply compatibility fix: {e}\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Try to load from .env file if available (optional)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass  # dotenv not installed, that's okay\n",
    "\n",
    "# Determine which backend to use\n",
    "USE_OLLAMA = os.environ.get(\"USE_OLLAMA\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "USE_GEMINI = os.environ.get(\"USE_GEMINI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    print(\"⚙️ Will use Ollama backend\")\n",
    "    assert os.getenv(\"OLLAMA_BASE_URL\") or True, \"⚠️ OLLAMA_BASE_URL optional (defaults to http://localhost:11434)\"\n",
    "elif USE_GEMINI:\n",
    "    print(\"⚙️ Will use Gemini backend\")\n",
    "    assert os.getenv(\"GOOGLE_API_KEY\"), \"⚠️ Please set GOOGLE_API_KEY in your environment!\"\n",
    "else:\n",
    "    print(\"⚙️ Will use OpenAI backend (default)\")\n",
    "    assert os.getenv(\"OPENAI_API_KEY\"), \"⚠️ Please set OPENAI_API_KEY in your environment!\"\n",
    "\n",
    "print(\"✅ Backend configuration OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Optional, Dict, Any, Union\n",
    "\n",
    "# Initialize the appropriate client based on backend selection\n",
    "if USE_OLLAMA:\n",
    "    # Use Ollama\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "    from langchain_core.language_models.chat_models import BaseChatModel\n",
    "    llm_backend: BaseChatModel = ChatOllama(model=\"mistral\", temperature=0.2)\n",
    "    client = None  # Ollama doesn't use OpenAI client\n",
    "    print(\"⚙️ Using Ollama backend (mistral)\")\n",
    "elif USE_GEMINI:\n",
    "    # Use Gemini\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "    client = genai  # Store genai module as client\n",
    "    llm_backend = None\n",
    "    print(\"⚙️ Using Gemini backend (gemini-2.5-flash)\")\n",
    "else:\n",
    "    # Use OpenAI (default)\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    llm_backend = None\n",
    "    print(\"⚙️ Using OpenAI backend (gpt-4o-mini)\")\n",
    "\n",
    "print(\"✅ Client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal A2A: Message Schema\n",
    "\n",
    "We'll use a compact message format inspired by A2A ideas:\n",
    "\n",
    "- `role`: \"system\" | \"user\" | \"assistant\" | \"tool\" | \"agent\"\n",
    "- `name`: optional identifier of the speaker agent\n",
    "- `content`: natural language content\n",
    "- `actions`: optional list of proposed actions (e.g., tool calls)\n",
    "- `metadata`: optional dict (e.g., routing hints)\n",
    "\n",
    "Agents exchange lists of these messages. A mediator (router) decides who speaks next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class A2AMessage(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\", \"tool\", \"agent\"]\n",
    "    content: str\n",
    "    name: Optional[str] = None\n",
    "    actions: Optional[List[Dict[str, Any]]] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class AgentConfig(BaseModel):\n",
    "    name: str\n",
    "    system_prompt: str = \"\"\n",
    "    model: str = \"gpt-4o-mini\"  # small, fast model suitable for classroom (or gemini-2.5-flash for Gemini)\n",
    "    temperature: float = 0.2\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    message: A2AMessage\n",
    "    stop: bool = False  # allow agent to signal completion\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, config: AgentConfig, client: Union[Any, None] = None, llm_backend: Union[Any, None] = None):\n",
    "        self.config = config\n",
    "        self.client = client\n",
    "        self.llm_backend = llm_backend\n",
    "        # Determine backend type\n",
    "        self.use_ollama = USE_OLLAMA\n",
    "        self.use_gemini = USE_GEMINI\n",
    "        self.use_openai = not USE_OLLAMA and not USE_GEMINI\n",
    "\n",
    "    def build_prompt(self, history: List[A2AMessage]) -> List[Dict[str, str]]:\n",
    "        messages: List[Dict[str, str]] = []\n",
    "        if self.config.system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": self.config.system_prompt})\n",
    "        for m in history:\n",
    "            # map A2A roles to chat roles where reasonable\n",
    "            role = m.role if m.role in {\"system\", \"user\", \"assistant\"} else \"user\"\n",
    "            name = f\"{m.name}: \" if m.name else \"\"\n",
    "            content = name + m.content\n",
    "            messages.append({\"role\": role, \"content\": content})\n",
    "        return messages\n",
    "\n",
    "    def step(self, history: List[A2AMessage]) -> AgentResponse:\n",
    "        messages = self.build_prompt(history)\n",
    "        \n",
    "        if self.use_ollama:\n",
    "            # Use LangChain ChatOllama\n",
    "            from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "            langchain_messages = []\n",
    "            for msg in messages:\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    langchain_messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "                elif msg[\"role\"] == \"user\":\n",
    "                    langchain_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "                elif msg[\"role\"] == \"assistant\":\n",
    "                    langchain_messages.append(AIMessage(content=msg[\"content\"]))\n",
    "            \n",
    "            response_obj = self.llm_backend.invoke(langchain_messages)\n",
    "            content = response_obj.content if hasattr(response_obj, 'content') else str(response_obj)\n",
    "            \n",
    "        elif self.use_gemini:\n",
    "            # Use Google Gemini\n",
    "            import google.generativeai as genai\n",
    "            # Convert messages to Gemini format\n",
    "            # Gemini uses a different format: list of dicts with \"role\" and \"parts\"\n",
    "            gemini_history = []\n",
    "            system_prompt_text = \"\"\n",
    "            last_user_message = \"\"\n",
    "            first_user_processed = False\n",
    "            \n",
    "            # Process messages: collect system prompt and build history\n",
    "            i = 0\n",
    "            while i < len(messages):\n",
    "                msg = messages[i]\n",
    "                if msg[\"role\"] == \"system\":\n",
    "                    system_prompt_text = msg[\"content\"]\n",
    "                    i += 1\n",
    "                elif msg[\"role\"] == \"user\":\n",
    "                    user_content = msg[\"content\"]\n",
    "                    # If this is the first user message and we have a system prompt, combine them\n",
    "                    if system_prompt_text and not first_user_processed:\n",
    "                        user_content = f\"{system_prompt_text}\\n\\n{user_content}\"\n",
    "                        system_prompt_text = \"\"  # Clear it since we've used it\n",
    "                        first_user_processed = True\n",
    "                    \n",
    "                    # Check if next message is assistant (to build history)\n",
    "                    if i + 1 < len(messages) and messages[i + 1][\"role\"] == \"assistant\":\n",
    "                        # Add user message and assistant response to history\n",
    "                        gemini_history.append({\"role\": \"user\", \"parts\": [user_content]})\n",
    "                        gemini_history.append({\"role\": \"model\", \"parts\": [messages[i + 1][\"content\"]]})\n",
    "                        i += 2  # Skip both user and assistant\n",
    "                    else:\n",
    "                        # This is the current prompt (last user message)\n",
    "                        last_user_message = user_content\n",
    "                        i += 1\n",
    "                elif msg[\"role\"] == \"assistant\":\n",
    "                    # This should have been handled with the previous user message\n",
    "                    i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            \n",
    "            # Use the configured model\n",
    "            model_name = self.config.model if self.config.model != \"gpt-4o-mini\" else \"gemini-2.5-flash\"\n",
    "            model = genai.GenerativeModel(\n",
    "                model_name=model_name,\n",
    "                generation_config={\"temperature\": self.config.temperature}\n",
    "            )\n",
    "            \n",
    "            # Use last user message as current prompt (system prompt already combined if needed)\n",
    "            current_prompt = last_user_message if last_user_message else system_prompt_text\n",
    "            \n",
    "            # Start a chat if we have history, otherwise generate directly\n",
    "            if gemini_history:\n",
    "                chat = model.start_chat(history=gemini_history)\n",
    "                response = chat.send_message(current_prompt)\n",
    "            else:\n",
    "                response = model.generate_content(current_prompt)\n",
    "            \n",
    "            content = response.text if hasattr(response, 'text') else str(response)\n",
    "            \n",
    "        else:\n",
    "            # Use OpenAI (default)\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.config.model,\n",
    "                temperature=self.config.temperature,\n",
    "                messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n",
    "            )\n",
    "            content = completion.choices[0].message.content or \"\"\n",
    "        \n",
    "        response = AgentResponse(\n",
    "            message=A2AMessage(role=\"agent\", name=self.config.name, content=content)\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two specialized agents that speak through the same A2A interface\n",
    "# Update model name for Gemini if using Gemini backend\n",
    "model_name = \"gemini-2.5-flash\" if USE_GEMINI else (\"mistral\" if USE_OLLAMA else \"gpt-4o-mini\")\n",
    "\n",
    "researcher = Agent(\n",
    "    AgentConfig(\n",
    "        name=\"Researcher\",\n",
    "        system_prompt=(\n",
    "            \"You are a helpful research assistant. \"\n",
    "            \"Summarize facts concisely, cite sources if possible. \"\n",
    "            \"When uncertain, ask clarifying questions.\"\n",
    "        ),\n",
    "        model=model_name,\n",
    "    ),\n",
    "    client=client,\n",
    "    llm_backend=llm_backend,\n",
    ")\n",
    "\n",
    "writer = Agent(\n",
    "    AgentConfig(\n",
    "        name=\"Writer\",\n",
    "        system_prompt=(\n",
    "            \"You are a clear technical writer. \"\n",
    "            \"Transform research notes into a polished, short paragraph.\"\n",
    "        ),\n",
    "        model=model_name,\n",
    "    ),\n",
    "    client=client,\n",
    "    llm_backend=llm_backend,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Route:\n",
    "    # Simple routing rule: after 'Researcher' speaks, 'Writer' responds; then stop\n",
    "    next_map: Dict[str, Optional[str]]\n",
    "\n",
    "    def next_speaker(self, current_name: str) -> Optional[str]:\n",
    "        return self.next_map.get(current_name)\n",
    "\n",
    "route = Route(next_map={\n",
    "    \"Researcher\": \"Writer\",\n",
    "    \"Writer\": None,  # stop after writer responds\n",
    "})\n",
    "\n",
    "history: List[A2AMessage] = [\n",
    "    A2AMessage(role=\"user\", name=\"Instructor\", content=\"Explain what Agent2Agent (A2A) is, briefly.\"),\n",
    "]\n",
    "\n",
    "# First, the Researcher responds\n",
    "resp_r = researcher.step(history)\n",
    "history.append(resp_r.message)\n",
    "print(f\"{resp_r.message.name}:\\n{resp_r.message.content}\\n\\n\")\n",
    "\n",
    "# Then route to Writer, who polishes the response\n",
    "next_name = route.next_speaker(\"Researcher\")\n",
    "if next_name == \"Writer\":\n",
    "    resp_w = writer.step(history)\n",
    "    history.append(resp_w.message)\n",
    "    print(f\"{resp_w.message.name}:\\n{resp_w.message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Add a Fact-Checker Agent\n",
    "\n",
    "- Create a third agent `FactChecker` whose role is to critique the Researcher output and ask for clarifications if needed.\n",
    "- Update the routing so that the turn order is: Researcher -> FactChecker -> Writer.\n",
    "- Keep the same `A2AMessage` history so all agents see the conversation context.\n",
    "\n",
    "Hint: copy the `Agent` instantiation pattern used for `researcher` and `writer` with a system prompt like: \"You verify factual claims, request sources, and highlight ambiguities.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Simple Tool-Use via A2A Actions\n",
    "\n",
    "We can simulate tool-use by allowing an agent to propose an action in `actions`, and a separate tool-runner to execute it. Below is a toy calculator tool the Researcher can call.\n",
    "\n",
    "Run once to define tool and an agent that may propose actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "TOOLS: Dict[str, Callable[..., Any]] = {}\n",
    "\n",
    "def tool(name: str):\n",
    "    def decorator(fn: Callable[..., Any]):\n",
    "        TOOLS[name] = fn\n",
    "        return fn\n",
    "    return decorator\n",
    "\n",
    "@tool(\"calculator\")\n",
    "def calculator(expr: str) -> str:\n",
    "    try:\n",
    "        # Danger: eval — keep to math-only by removing builtins\n",
    "        result = eval(expr, {\"__builtins__\": {}}, {\"sqrt\": sqrt})\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "class ToolAwareAgent(Agent):\n",
    "    def step(self, history: List[A2AMessage]) -> AgentResponse:\n",
    "        # Let the LLM propose an action via a protocol hint\n",
    "        prompt_hint = (\n",
    "            \"If you need to compute something, propose an action in JSON as: \"\n",
    "            \"ACTION: {\\\"tool\\\": \\\"calculator\\\", \\\"input\\\": \\\"...\\\"}. \"\n",
    "            \"Otherwise, answer normally.\"\n",
    "        )\n",
    "        extended = history + [A2AMessage(role=\"system\", content=prompt_hint)]\n",
    "        resp = super().step(extended)\n",
    "\n",
    "        # crude parse for ACTION: {...}\n",
    "        import re, json\n",
    "        match = re.search(r\"ACTION:\\s*(\\{.*\\})\", resp.message.content, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                action = json.loads(match.group(1))\n",
    "                tool_name = action.get(\"tool\")\n",
    "                tool_input = action.get(\"input\", \"\")\n",
    "                if tool_name in TOOLS:\n",
    "                    tool_result = TOOLS[tool_name](tool_input)\n",
    "                    tool_msg = A2AMessage(role=\"tool\", name=tool_name, content=str(tool_result))\n",
    "                    # Have the agent incorporate the tool result\n",
    "                    follow_up = super().step(history + [resp.message, tool_msg])\n",
    "                    return follow_up\n",
    "            except Exception:\n",
    "                pass\n",
    "        return resp\n",
    "\n",
    "calc_researcher = ToolAwareAgent(\n",
    "    AgentConfig(\n",
    "        name=\"Researcher\",\n",
    "        system_prompt=(\n",
    "            \"You are a research assistant who can optionally use a calculator tool. \"\n",
    "            \"When asked to compute, propose an ACTION with a simple expression e.g. '2+2' or 'sqrt(9)'.\"\n",
    "        ),\n",
    "        model=model_name,\n",
    "    ),\n",
    "    client=client,\n",
    "    llm_backend=llm_backend,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the tool-aware researcher a question that requires computation\n",
    "history_calc: List[A2AMessage] = [\n",
    "    A2AMessage(role=\"user\", name=\"Instructor\", content=\"What is sqrt(144) + 10? Answer with reasoning and final number.\"),\n",
    "]\n",
    "resp_calc = calc_researcher.step(history_calc)\n",
    "print(f\"{resp_calc.message.name}:\\n{resp_calc.message.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate FactChecker and run Researcher -> FactChecker -> Writer\n",
    "fact_checker = Agent(\n",
    "    AgentConfig(\n",
    "        name=\"FactChecker\",\n",
    "        system_prompt=(\n",
    "            \"You verify factual claims, request sources, and highlight ambiguities. \"\n",
    "            \"Be concise and constructive; suggest corrections or needed citations.\"\n",
    "        ),\n",
    "        model=model_name,\n",
    "    ),\n",
    "    client=client,\n",
    "    llm_backend=llm_backend,\n",
    ")\n",
    "\n",
    "# New route order\n",
    "route_fc = {\n",
    "    \"Researcher\": \"FactChecker\",\n",
    "    \"FactChecker\": \"Writer\",\n",
    "    \"Writer\": None,\n",
    "}\n",
    "\n",
    "def run_with_fact_checker(user_prompt: str) -> None:\n",
    "    convo: List[A2AMessage] = [\n",
    "        A2AMessage(role=\"user\", name=\"Instructor\", content=user_prompt)\n",
    "    ]\n",
    "\n",
    "    # Researcher turn\n",
    "    resp_r = researcher.step(convo)\n",
    "    convo.append(resp_r.message)\n",
    "    print(f\"{resp_r.message.name}:\\n{resp_r.message.content}\\n\\n\")\n",
    "\n",
    "    # FactChecker turn\n",
    "    if route_fc[\"Researcher\"] == \"FactChecker\":\n",
    "        resp_f = fact_checker.step(convo)\n",
    "        convo.append(resp_f.message)\n",
    "        print(f\"{resp_f.message.name}:\\n{resp_f.message.content}\\n\\n\")\n",
    "\n",
    "    # Writer turn\n",
    "    if route_fc[\"FactChecker\"] == \"Writer\":\n",
    "        resp_w = writer.step(convo)\n",
    "        convo.append(resp_w.message)\n",
    "        print(f\"{resp_w.message.name}:\\n{resp_w.message.content}\")\n",
    "\n",
    "# Example run\n",
    "run_with_fact_checker(\"Explain what Agent2Agent (A2A) is, briefly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
