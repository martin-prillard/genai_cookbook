{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Techniques with Qdrant\n",
    "\n",
    "**Author:** Advanced RAG Implementation Guide\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "\n",
    "This notebook provides a **comprehensive reference implementation** of advanced RAG (Retrieval-Augmented Generation) techniques. It demonstrates how to build sophisticated retrieval pipelines that go beyond basic vector similarity search, enabling more accurate and contextually rich AI applications.\n",
    "\n",
    "### ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to implement multi-level document chunking strategies\n",
    "- Techniques for improving retrieval relevance through reranking\n",
    "- Advanced query processing methods (expansion, fusion, planning)\n",
    "- Evaluation methodologies for RAG systems\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Features\n",
    "\n",
    "This notebook covers the following **advanced RAG techniques**:\n",
    "\n",
    "### 1. **Parent/Child Chunking** via `ParentDocumentRetriever`\n",
    "   - âœ… Enables retrieving precise passages while maintaining broader context\n",
    "   - âœ… Improves both retrieval precision and generation quality\n",
    "   - âœ… Reduces context fragmentation in final answers\n",
    "\n",
    "### 2. **LLM-based Reranking**\n",
    "   - âœ… Uses an LLM to score and rerank retrieved documents\n",
    "   - âœ… Improves relevance of final context passed to the generator\n",
    "   - âœ… Filters out irrelevant documents before generation\n",
    "\n",
    "### 3. **RAG-Fusion** (Query Expansion + Reciprocal Rank Fusion)\n",
    "   - âœ… Generates multiple query reformulations\n",
    "   - âœ… Merges results using Reciprocal Rank Fusion (RRF) algorithm\n",
    "   - âœ… Significantly improves recall on heterogeneous corpora\n",
    "   - âœ… Handles ambiguous or multi-faceted queries effectively\n",
    "\n",
    "### 4. **Query Planning** (Minimal Plan-and-Execute)\n",
    "   - âœ… Decomposes complex questions into atomic sub-questions\n",
    "   - âœ… Executes each step sequentially with appropriate tools\n",
    "   - âœ… Synthesizes results into a coherent final answer\n",
    "   - âœ… Enables multi-step reasoning workflows\n",
    "\n",
    "### 5. **Evaluation Framework** (LLM-as-a-Judge)\n",
    "   - âœ… Evaluates answer quality using an LLM judge\n",
    "   - âœ… Measures factual accuracy, completeness, and clarity\n",
    "   - âœ… Includes comparative analysis between different pipeline configurations\n",
    "   - âœ… Provides quantitative metrics for pipeline optimization\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Prerequisites\n",
    "\n",
    "### Required Environment Variables\n",
    "- `OPENAI_API_KEY` - Your OpenAI API key (must be set)\n",
    "\n",
    "### Required Python Packages\n",
    "```bash\n",
    "pip install langchain langchain-openai langchain-community qdrant-client tiktoken ragas pypdf networkx rank-bm25\n",
    "```\n",
    "\n",
    "**Note:** The notebook includes an installation cell, but ensure you have these packages installed before running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation\n",
    "\n",
    "Install the required packages for this notebook. Run this cell if you haven't already installed the dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain-openai langchain-community qdrant-client tiktoken ragas pypdf networkx rank-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Environment Setup\n",
    "\n",
    "Verify that your OpenAI API key is configured. This notebook requires an active `OPENAI_API_KEY` environment variable to function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OK: OPENAI_API_KEY detected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Try to load from .env file if available (optional)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass  # dotenv not installed, that's okay\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"âš ï¸ Please set OPENAI_API_KEY in your environment!\"\n",
    "print(\"âœ… OK: OPENAI_API_KEY detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Imports and Dependencies\n",
    "\n",
    "Import all necessary libraries for building advanced RAG pipelines:\n",
    "- **LangChain components**: Document handling, text splitters, embeddings, vector stores\n",
    "- **Qdrant**: Vector database for storing embeddings\n",
    "- **OpenAI**: Embeddings and LLM models\n",
    "- **Core utilities**: Type hints, JSON handling, itertools for result fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import itertools\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_classic.retrievers.parent_document_retriever import ParentDocumentRetriever\n",
    "from langchain_classic.storage import InMemoryStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Example Corpus Setup\n",
    "\n",
    "Create a small demonstration corpus with sample documents about RAG techniques. This corpus will be used throughout the notebook to demonstrate various advanced RAG methods.\n",
    "\n",
    "**What this section does:**\n",
    "- Creates sample documents about RAG techniques\n",
    "- Prepares text splitters for parent and child chunking\n",
    "- Sets up the document structure for the parent/child retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raws = [\n",
    "    \"\"\"Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local office. Unfortunately, during the interview, he fell asleep on the keyboard, accidentally sending a company-wide email that said, 'Meow if you approve!' Surprisingly, the staff loved it, thinking it was a new motivational strategy. Mr. Jonas got the promotion, now supervising the office naps. Soon, he implemented 'cat breaks' every hour, where employees were encouraged to stretch, purr softly, or chase imaginary laser pointers. Productivity oddly skyrocketed, and the office became famous for its feline-inspired efficiency seminars. Mr. Jonas even started a newsletter titled 'The Purr-fect Manager,' sharing tips on balancing work and naps.\"\"\",\n",
    "    \"\"\"Jenny bought a toaster from a mysterious yard sale, not realizing it was a time machine. Every time she toasted bread, it popped up with tiny messages from the future. One morning, the toast said, 'Beware the dancing squirrels at 3 PM!' She ignored it, and sure enough, a flash mob of squirrels waltzed through her garden, all wearing tiny top hats. Jenny now always eats her breakfast with a helmet. Eventually, she discovered a secret lever inside the toaster that allowed her to send messages back in time. She used it to warn her past self about spilled coffee, missed deadlines, and unexpected squirrel invasions. Breakfast became an adventure, with toast revealing mysteries of the universe and sometimes giving advice on what socks to wear for luck.\"\"\",\n",
    "    \"\"\"In the small town of Noodleton, spaghetti rained from the sky for no reason at all. Mayor Luigi tried to stop it with umbrellas, but the noodles were relentless. One ambitious dog named Max decided to chase the falling spaghetti like a game. Soon, the whole town joined, slipping, sliding, and twirling in a chaotic noodle dance-off. By evening, everyone agreed: it was the tastiest disaster in history. Over the next few days, chefs experimented with sky-spaghetti recipes, inventing noodle ice cream, spaghetti smoothies, and pasta hats. Scientists tried to figure out the phenomenon but concluded it was 'spaghetti magic.' Max became the town hero, leading annual 'noodle festivals,' and Noodleton transformed into the world's first pasta-themed tourist destination.\"\"\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t, metadata={\"source\": \"demo\", \"idx\": i}) for i, t in enumerate(raws)]\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=80)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=40)\n",
    "parents = parent_splitter.split_documents(docs)\n",
    "children = []  # Children will be generated by the retriever; we still prepare the structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Parent/Child Chunking Implementation\n",
    "\n",
    "The **ParentDocumentRetriever** implements a sophisticated two-level chunking strategy that addresses the classic trade-off between retrieval precision and context completeness.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "- **Child chunks** (smaller, ~250 tokens)\n",
    "  - Used for initial retrieval via vector similarity search\n",
    "  - Provides fine-grained matching for precise passage retrieval\n",
    "  - Better semantic alignment with specific query aspects\n",
    "  - Enables finding needle-in-haystack information\n",
    "  \n",
    "- **Parent chunks** (larger, ~800 tokens)\n",
    "  - Retrieved automatically when a child chunk matches\n",
    "  - Provides broader context around the matched passage\n",
    "  - Reduces context fragmentation and improves answer coherence\n",
    "  - Maintains narrative flow and related information\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "âœ… **Ideal for:**\n",
    "- Documents with dense information where context matters\n",
    "- Long-form content (articles, reports, documentation)\n",
    "- Scenarios requiring both precision and completeness\n",
    "- Applications where answer quality depends on surrounding context\n",
    "\n",
    "âŒ **Less suitable for:**\n",
    "- Very short documents or FAQ-style content\n",
    "- Real-time systems with strict latency requirements\n",
    "- Simple Q&A where exact matches are sufficient\n",
    "\n",
    "\n",
    "**What this section does:**\n",
    "- Initializes OpenAI embeddings (`text-embedding-3-small`)\n",
    "- Creates a Qdrant vector store (in-memory for this demo)\n",
    "- Sets up the `ParentDocumentRetriever` with:\n",
    "  - **Child splitter**: Small chunks (~250 tokens) for precise retrieval\n",
    "  - **Parent splitter**: Larger chunks (~800 tokens) for context\n",
    "- Indexes the documents using the parent/child strategy\n",
    "\n",
    "**Key Concept:** The retriever stores child chunks in the vector store for retrieval, but returns parent chunks to provide broader context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/05/3bcz94ds2xlfpypg943txflc0000gn/T/ipykernel_32652/2257423038.py:26: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-qdrant package and should be used instead. To use it run `pip install -U `langchain-qdrant` and import as `from `langchain_qdrant import Qdrant``.\n",
      "  vectorstore = Qdrant(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create an empty Qdrant vectorstore for ParentDocumentRetriever\n",
    "# We'll use in-memory storage with a unique collection name\n",
    "# ParentDocumentRetriever will handle adding documents to the vectorstore\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "collection_name = \"parent_child_docs\"\n",
    "\n",
    "# Create the vectorstore - we need to create it with from_texts to initialize the collection\n",
    "# Using an empty list to create the collection structure\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "try:\n",
    "    # Try to get the collection, if it doesn't exist, create it\n",
    "    client.get_collection(collection_name)\n",
    "except:\n",
    "    # Collection doesn't exist, create it\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(\n",
    "            size=1536,  # text-embedding-3-small dimension\n",
    "            distance=Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Now create the vectorstore\n",
    "vectorstore = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ LLM-based Reranking\n",
    "\n",
    "Implement reranking using an LLM to score document relevance. This improves retrieval quality by filtering out irrelevant documents before generation.\n",
    "\n",
    "**What this section does:**\n",
    "- Defines a scoring prompt that asks the LLM to rate document relevance (0-100)\n",
    "- Implements `rerank_llm()` function that:\n",
    "  1. Scores each document for relevance to the query\n",
    "  2. Sorts documents by score\n",
    "  3. Returns the top-k most relevant documents\n",
    "\n",
    "**Benefits:**\n",
    "- Better document selection than pure vector similarity\n",
    "- Reduces noise in the context passed to the generator\n",
    "- Improves answer quality by focusing on truly relevant passages\n",
    "\n",
    "**Note:** This is computationally expensive as it requires one LLM call per document. Consider batching or using dedicated reranking models for production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def score_prompt():\n",
    "    return ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an evaluator. Rate from 0 to 100 the relevance of the PASSAGE for answering the QUESTION. \"\n",
    "                   \"Respond only with an integer.\"),\n",
    "        (\"human\", \"QUESTION:\\n{question}\\n\\nPASSAGE:\\n{passage}\\n\\nScore (0-100):\")\n",
    "    ])\n",
    "\n",
    "async def score_passage_async(passage: str, question: str):\n",
    "    # Use LCEL pattern for LangChain 1.0 compatibility\n",
    "    prompt_template = score_prompt()\n",
    "    chain = prompt_template | llm\n",
    "    resp = await chain.ainvoke({\"question\": question, \"passage\": passage})\n",
    "    try:\n",
    "        return int(''.join(ch for ch in resp.content if ch.isdigit())[:3] or 0)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def rerank_llm(passages: List[Document], question: str, k: int = 3) -> List[Document]:\n",
    "    # Synchronous call for simplicity (can be async with asyncio.gather)\n",
    "    scored = []\n",
    "    prompt_template = score_prompt()\n",
    "    # Use LCEL pattern for LangChain 1.0 compatibility\n",
    "    chain = prompt_template | llm\n",
    "    for d in passages:\n",
    "        resp = chain.invoke({\"question\": question, \"passage\": d.page_content[:1500]})\n",
    "        try:\n",
    "            s = int(''.join(ch for ch in resp.content if ch.isdigit())[:3] or 0)\n",
    "        except Exception:\n",
    "            s = 0\n",
    "        scored.append((s, d))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return [d for _, d in scored[:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ RAG Pipeline Funnel: From BM25 to Final Reranking\n",
    "\n",
    "This section demonstrates the **funnel operation** of a multi-stage RAG pipeline, showing how documents are progressively filtered and refined from initial frequency-based retrieval to final reranking.\n",
    "\n",
    "#### The Funnel Concept\n",
    "\n",
    "A RAG funnel progressively narrows down the document set through multiple retrieval stages:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 1: BM25 (Frequency-based)        â”‚\n",
    "â”‚  â†’ Broad retrieval using term frequency â”‚\n",
    "â”‚  â†’ Returns: ~20-30 documents            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 2: Vector Similarity (Semantic)   â”‚\n",
    "â”‚  â†’ Semantic matching using embeddings   â”‚\n",
    "â”‚  â†’ Returns: ~8-10 documents             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "              â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Stage 3: LLM Reranking                  â”‚\n",
    "â”‚  â†’ Contextual relevance scoring          â”‚\n",
    "â”‚  â†’ Returns: ~3-5 final documents         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### Why Use a Funnel?\n",
    "\n",
    "**Advantages:**\n",
    "- âœ… **Efficiency**: BM25 is fast and catches keyword matches\n",
    "- âœ… **Recall**: Initial broad retrieval ensures nothing is missed\n",
    "- âœ… **Precision**: Each stage refines results for better relevance\n",
    "- âœ… **Cost Optimization**: Expensive LLM reranking only on top candidates\n",
    "- âœ… **Hybrid Approach**: Combines lexical (BM25) and semantic (vector) signals\n",
    "\n",
    "**Trade-offs:**\n",
    "- âš ï¸ More complex pipeline with multiple stages\n",
    "- âš ï¸ Requires tuning of k values at each stage\n",
    "- âš ï¸ May filter out relevant documents if stages are too aggressive\n",
    "\n",
    "**What this section does:**\n",
    "- Implements BM25 retrieval using frequency-based term matching\n",
    "- Demonstrates the funnel operation with visual output\n",
    "- Shows document counts and snippets at each stage\n",
    "- Compares results across different retrieval methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”„ RAG PIPELINE FUNNEL DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Œ Query: What happened to Mr. Jonas at the office?\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š STAGE 1: BM25 (Frequency-based Retrieval)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: Term frequency + Inverse document frequency (TF-IDF variant)\n",
      "Approach: Lexical matching - finds documents with query keywords\n",
      "Retrieval size: Top 20 documents\n",
      "\n",
      "âœ… Retrieved 12 documents via BM25\n",
      "\n",
      "Top 5 BM25 results (showing snippets):\n",
      "  1. became an adventure, with toast revealing mysteries of the universe and sometimes giving advice on what socks to wear fo...\n",
      "  2. Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local of...\n",
      "  3. he implemented 'cat breaks' every hour, where employees were encouraged to stretch, purr softly, or chase imaginary lase...\n",
      "  4. sending a company-wide email that said, 'Meow if you approve!' Surprisingly, the staff loved it, thinking it was a new m...\n",
      "  5. efficiency seminars. Mr. Jonas even started a newsletter titled 'The Purr-fect Manager,' sharing tips on balancing work ...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ” STAGE 2: Vector Similarity (Semantic Retrieval)\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: Embedding-based cosine similarity\n",
      "Approach: Semantic matching - understands meaning and context\n",
      "Retrieval size: Top 8 documents\n",
      "\n",
      "âœ… Retrieved 1 documents via vector similarity\n",
      "\n",
      "Top 1 vector results (showing snippets):\n",
      "  1. Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local of...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ† STAGE 3: LLM-based Reranking\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Method: LLM contextual relevance scoring\n",
      "Approach: Deep understanding of query-document relationship\n",
      "Input: Top 1 from vector search\n",
      "Output: Top 3 most relevant documents\n",
      "\n",
      "âœ… Reranked to 1 final documents\n",
      "\n",
      "Final top 1 reranked results:\n",
      "  1. Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local office. Unfortunately, during th...\n",
      "\n",
      "======================================================================\n",
      "ðŸ“ˆ FUNNEL SUMMARY\n",
      "======================================================================\n",
      "Stage 1 (BM25):         12 documents â†’ Frequency-based keyword matching\n",
      "Stage 2 (Vector):        1 documents â†’ Semantic similarity filtering\n",
      "Stage 3 (Reranking):     1 documents â†’ Contextual relevance scoring\n",
      "\n",
      "Reduction: 12 â†’ 1 â†’ 1\n",
      "Final precision: 1/12 = 8.3% of initial set\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare documents for BM25 (need tokenized text)\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization for BM25\"\"\"\n",
    "    # Convert to lowercase and split on whitespace/punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokens\n",
    "\n",
    "# Get all child chunks from the retriever for BM25 indexing\n",
    "# We'll use the parent documents and split them for BM25\n",
    "all_docs_for_bm25 = []\n",
    "for doc in docs:\n",
    "    # Split each document into smaller chunks for BM25\n",
    "    chunks = child_splitter.split_documents([doc])\n",
    "    all_docs_for_bm25.extend(chunks)\n",
    "\n",
    "# Tokenize all documents for BM25\n",
    "tokenized_docs = [tokenize(doc.page_content) for doc in all_docs_for_bm25]\n",
    "\n",
    "# Create BM25 index\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "def retrieve_bm25(query: str, k: int = 20) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using BM25 (frequency-based approach).\n",
    "    \n",
    "    BM25 (Best Matching 25) is a ranking function that scores documents\n",
    "    based on term frequency (TF) and inverse document frequency (IDF).\n",
    "    It's particularly good at matching exact keywords and phrases.\n",
    "    \"\"\"\n",
    "    query_tokens = tokenize(query)\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top k documents\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if scores[idx] > 0:  # Only include documents with non-zero scores\n",
    "            results.append(all_docs_for_bm25[idx])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def retrieve_vector(query: str, k: int = 10) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Retrieve documents using vector similarity (semantic search).\n",
    "    \n",
    "    Uses the parent/child retriever which performs semantic matching\n",
    "    using embeddings. This captures meaning and context beyond keywords.\n",
    "    \"\"\"\n",
    "    return retriever.invoke(query)[:k]\n",
    "\n",
    "def demonstrate_rag_funnel(question: str, bm25_k: int = 20, vector_k: int = 8, rerank_k: int = 3):\n",
    "    \"\"\"\n",
    "    Demonstrate the RAG funnel operation showing progressive filtering.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. BM25 retrieval (frequency-based) â†’ broad set\n",
    "    2. Vector similarity (semantic) â†’ refined set  \n",
    "    3. LLM reranking â†’ final top documents\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ”„ RAG PIPELINE FUNNEL DEMONSTRATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ“Œ Query: {question}\\n\")\n",
    "    \n",
    "    # Stage 1: BM25 (Frequency-based retrieval)\n",
    "    print(\"â”€\"*70)\n",
    "    print(\"ðŸ“Š STAGE 1: BM25 (Frequency-based Retrieval)\")\n",
    "    print(\"â”€\"*70)\n",
    "    print(f\"Method: Term frequency + Inverse document frequency (TF-IDF variant)\")\n",
    "    print(f\"Approach: Lexical matching - finds documents with query keywords\")\n",
    "    print(f\"Retrieval size: Top {bm25_k} documents\\n\")\n",
    "    \n",
    "    bm25_results = retrieve_bm25(question, k=bm25_k)\n",
    "    print(f\"âœ… Retrieved {len(bm25_results)} documents via BM25\")\n",
    "    print(f\"\\nTop 5 BM25 results (showing snippets):\")\n",
    "    for i, doc in enumerate(bm25_results[:5], 1):\n",
    "        snippet = doc.page_content[:120].replace(\"\\n\", \" \") + \"...\"\n",
    "        print(f\"  {i}. {snippet}\")\n",
    "    \n",
    "    # Stage 2: Vector Similarity (Semantic retrieval)\n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"ðŸ” STAGE 2: Vector Similarity (Semantic Retrieval)\")\n",
    "    print(\"â”€\"*70)\n",
    "    print(f\"Method: Embedding-based cosine similarity\")\n",
    "    print(f\"Approach: Semantic matching - understands meaning and context\")\n",
    "    print(f\"Retrieval size: Top {vector_k} documents\\n\")\n",
    "    \n",
    "    vector_results = retrieve_vector(question, k=vector_k)\n",
    "    print(f\"âœ… Retrieved {len(vector_results)} documents via vector similarity\")\n",
    "    print(f\"\\nTop {min(5, len(vector_results))} vector results (showing snippets):\")\n",
    "    for i, doc in enumerate(vector_results[:5], 1):\n",
    "        snippet = doc.page_content[:120].replace(\"\\n\", \" \") + \"...\"\n",
    "        print(f\"  {i}. {snippet}\")\n",
    "    \n",
    "    # Stage 3: Reranking\n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"ðŸ† STAGE 3: LLM-based Reranking\")\n",
    "    print(\"â”€\"*70)\n",
    "    print(f\"Method: LLM contextual relevance scoring\")\n",
    "    print(f\"Approach: Deep understanding of query-document relationship\")\n",
    "    print(f\"Input: Top {len(vector_results)} from vector search\")\n",
    "    print(f\"Output: Top {rerank_k} most relevant documents\\n\")\n",
    "    \n",
    "    # Rerank the vector results\n",
    "    reranked_results = rerank_llm(vector_results, question, k=rerank_k)\n",
    "    print(f\"âœ… Reranked to {len(reranked_results)} final documents\")\n",
    "    print(f\"\\nFinal top {len(reranked_results)} reranked results:\")\n",
    "    for i, doc in enumerate(reranked_results, 1):\n",
    "        snippet = doc.page_content[:150].replace(\"\\n\", \" \") + \"...\"\n",
    "        print(f\"  {i}. {snippet}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“ˆ FUNNEL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Stage 1 (BM25):        {len(bm25_results):3d} documents â†’ Frequency-based keyword matching\")\n",
    "    print(f\"Stage 2 (Vector):      {len(vector_results):3d} documents â†’ Semantic similarity filtering\")\n",
    "    print(f\"Stage 3 (Reranking):   {len(reranked_results):3d} documents â†’ Contextual relevance scoring\")\n",
    "    print(f\"\\nReduction: {len(bm25_results)} â†’ {len(vector_results)} â†’ {len(reranked_results)}\")\n",
    "    print(f\"Final precision: {len(reranked_results)}/{len(bm25_results)} = {len(reranked_results)/len(bm25_results)*100:.1f}% of initial set\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"bm25\": bm25_results,\n",
    "        \"vector\": vector_results,\n",
    "        \"reranked\": reranked_results\n",
    "    }\n",
    "\n",
    "# Example demonstration\n",
    "results = demonstrate_rag_funnel(\"What happened to Mr. Jonas at the office?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”€ Hybrid Retrieval: Combining BM25 and Vector Search\n",
    "\n",
    "In practice, many production RAG systems combine BM25 and vector search results for optimal performance. This hybrid approach leverages both lexical (keyword) and semantic (meaning) signals.\n",
    "\n",
    "**Benefits of Hybrid Approach:**\n",
    "- âœ… **Better Recall**: Catches both exact keyword matches (BM25) and semantic matches (vector)\n",
    "- âœ… **Robustness**: Handles queries that benefit from either approach\n",
    "- âœ… **Complementary Signals**: BM25 excels at specific terms, vectors excel at concepts\n",
    "\n",
    "**Common Fusion Strategies:**\n",
    "1. **Reciprocal Rank Fusion (RRF)**: Merge rankings from both methods\n",
    "2. **Weighted Combination**: Weighted average of BM25 and vector scores\n",
    "3. **Union then Rerank**: Take union of results, then rerank\n",
    "\n",
    "Let's demonstrate a hybrid approach using RRF to combine BM25 and vector results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ”€ HYBRID RETRIEVAL: BM25 + Vector Search\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Œ Query: What unusual events happened in these stories?\n",
      "\n",
      "ðŸ“Š BM25 Results:    4 documents\n",
      "ðŸ” Vector Results:  2 documents\n",
      "ðŸ”— Hybrid Results:  5 documents (after RRF fusion)\n",
      "\n",
      "Top hybrid results (BM25 + Vector fusion):\n",
      "  1. In the small town of Noodleton, spaghetti rained from the sky for no reason at all. Mayor Luigi tried to stop it with um...\n",
      "  2. became an adventure, with toast revealing mysteries of the universe and sometimes giving advice on what socks to wear fo...\n",
      "  3. Jenny bought a toaster from a mysterious yard sale, not realizing it was a time machine. Every time she toasted bread, i...\n",
      "  4. falling spaghetti like a game. Soon, the whole town joined, slipping, sliding, and twirling in a chaotic noodle dance-of...\n",
      "  5. discovered a secret lever inside the toaster that allowed her to send messages back in time. She used it to warn her pas...\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“ˆ Comparison Summary\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "BM25 only:        4 documents\n",
      "Vector only:      2 documents\n",
      "Hybrid (RRF):     5 documents\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hybrid_retrieval_bm25_vector(query: str, bm25_k: int = 15, vector_k: int = 10, final_k: int = 8):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining BM25 and vector search using Reciprocal Rank Fusion.\n",
    "    \n",
    "    This demonstrates how to combine frequency-based and semantic retrieval\n",
    "    for better overall performance.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ”€ HYBRID RETRIEVAL: BM25 + Vector Search\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nðŸ“Œ Query: {query}\\n\")\n",
    "    \n",
    "    # Get results from both methods\n",
    "    bm25_results = retrieve_bm25(query, k=bm25_k)\n",
    "    vector_results = retrieve_vector(query, k=vector_k)\n",
    "    \n",
    "    print(f\"ðŸ“Š BM25 Results:    {len(bm25_results)} documents\")\n",
    "    print(f\"ðŸ” Vector Results:  {len(vector_results)} documents\")\n",
    "    \n",
    "    # Combine using RRF\n",
    "    # Create a simple RRF implementation for two result sets\n",
    "    def simple_rrf(list1: List[Document], list2: List[Document], k: int = 60) -> List[Document]:\n",
    "        scores = {}\n",
    "        seen = set()\n",
    "        \n",
    "        # Score documents from first list (BM25)\n",
    "        for rank, doc in enumerate(list1, start=1):\n",
    "            key = (doc.metadata.get(\"source\"), doc.metadata.get(\"idx\"), doc.page_content[:50])\n",
    "            if key not in seen:\n",
    "                scores[key] = scores.get(key, 0) + 1.0 / (k + rank)\n",
    "                seen.add(key)\n",
    "        \n",
    "        # Score documents from second list (Vector)\n",
    "        for rank, doc in enumerate(list2, start=1):\n",
    "            key = (doc.metadata.get(\"source\"), doc.metadata.get(\"idx\"), doc.page_content[:50])\n",
    "            if key not in seen:\n",
    "                scores[key] = scores.get(key, 0) + 1.0 / (k + rank)\n",
    "                seen.add(key)\n",
    "            else:\n",
    "                # Document appears in both lists - boost its score\n",
    "                scores[key] += 1.0 / (k + rank)\n",
    "        \n",
    "        # Sort by score\n",
    "        ranked = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        \n",
    "        # Reconstruct documents\n",
    "        all_docs = list1 + list2\n",
    "        result = []\n",
    "        seen_keys = set()\n",
    "        for key, score in ranked[:final_k]:\n",
    "            for doc in all_docs:\n",
    "                doc_key = (doc.metadata.get(\"source\"), doc.metadata.get(\"idx\"), doc.page_content[:50])\n",
    "                if doc_key == key and doc_key not in seen_keys:\n",
    "                    result.append(doc)\n",
    "                    seen_keys.add(doc_key)\n",
    "                    break\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    hybrid_results = simple_rrf(bm25_results, vector_results, k=60)\n",
    "    \n",
    "    print(f\"ðŸ”— Hybrid Results:  {len(hybrid_results)} documents (after RRF fusion)\\n\")\n",
    "    \n",
    "    print(\"Top hybrid results (BM25 + Vector fusion):\")\n",
    "    for i, doc in enumerate(hybrid_results[:5], 1):\n",
    "        snippet = doc.page_content[:120].replace(\"\\n\", \" \") + \"...\"\n",
    "        print(f\"  {i}. {snippet}\")\n",
    "    \n",
    "    print(\"\\n\" + \"â”€\"*70)\n",
    "    print(\"ðŸ“ˆ Comparison Summary\")\n",
    "    print(\"â”€\"*70)\n",
    "    print(f\"BM25 only:        {len(bm25_results)} documents\")\n",
    "    print(f\"Vector only:      {len(vector_results)} documents\")\n",
    "    print(f\"Hybrid (RRF):     {len(hybrid_results)} documents\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    return hybrid_results\n",
    "\n",
    "# Demonstrate hybrid retrieval\n",
    "hybrid_docs = hybrid_retrieval_bm25_vector(\"What unusual events happened in these stories?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ RAG-Fusion Implementation\n",
    "\n",
    "Implement **RAG-Fusion**, a technique that combines query expansion with Reciprocal Rank Fusion (RRF) to significantly improve recall. **RAG-Fusion** is a powerful technique that combines query expansion with result fusion to significantly improve recall, especially on heterogeneous or complex corpora.\n",
    "\n",
    "#### Core Components\n",
    "\n",
    "**1. Query Expansion**\n",
    "- Generates diverse reformulations of the original question\n",
    "- Captures different phrasings, perspectives, and semantic variations\n",
    "- Helps retrieve documents that might be missed by the original query\n",
    "- Example: \"What is RAG?\" â†’ [\"Explain RAG\", \"How does retrieval-augmented generation work?\", \"Describe RAG systems\"]\n",
    "\n",
    "**2. Reciprocal Rank Fusion (RRF)**\n",
    "- Merges results from multiple queries using a mathematical formula\n",
    "- **Formula**: `score = Î£(1 / (k + rank))` across all query results\n",
    "- Documents appearing in multiple result sets receive higher scores\n",
    "- Naturally promotes consensus documents (documents relevant to multiple query variations)\n",
    "- The constant `k` (typically 60) prevents division by zero and smooths the ranking\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "âœ… **Particularly effective for:**\n",
    "- Heterogeneous corpora with varied document types\n",
    "- Ambiguous queries that could be interpreted multiple ways\n",
    "- Multi-domain knowledge bases\n",
    "- Scenarios where recall is more important than precision\n",
    "- Complex questions requiring information from multiple perspectives\n",
    "\n",
    "âŒ **May be overkill for:**\n",
    "- Simple, well-defined queries\n",
    "- Homogeneous document collections\n",
    "- Systems with strict latency constraints\n",
    "- Very specific, unambiguous questions\n",
    "\n",
    "\n",
    "**What this section does:**\n",
    "\n",
    "### Query Expansion\n",
    "- `query_expansions()`: Generates multiple reformulations of the original query\n",
    "- Uses an LLM to create diverse phrasings that capture different aspects of the question\n",
    "- Helps retrieve documents that might be missed by the original query\n",
    "\n",
    "### Reciprocal Rank Fusion (RRF)\n",
    "- `reciprocal_rank_fusion()`: Merges results from multiple queries\n",
    "- **Formula**: `score = Î£(1 / (k + rank))` across all query result sets\n",
    "- Documents appearing in multiple result sets get higher scores\n",
    "- Naturally promotes consensus documents (documents relevant to multiple query variations)\n",
    "\n",
    "### Search Function\n",
    "- `search_once()`: Performs a single retrieval using the parent/child retriever\n",
    "\n",
    "**Benefits:**\n",
    "- Significantly improves recall on heterogeneous corpora\n",
    "- Handles ambiguous queries effectively\n",
    "- Captures information from multiple perspectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_expansions(question: str, n: int = 4) -> List[str]:\n",
    "    tmpl = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant who proposes short and diverse reformulations of a question.\"),\n",
    "        (\"human\", \"Question: {q}\\nGive {n} different reformulations, one per line, without numbers.\")\n",
    "    ])\n",
    "    # Use LCEL pattern for LangChain 1.0 compatibility\n",
    "    chain = tmpl | llm\n",
    "    resp = chain.invoke({\"q\": question, \"n\": n})\n",
    "    lines = [ln.strip(\"- â€¢* \").strip() for ln in resp.content.splitlines() if ln.strip()]\n",
    "    return lines[:n] if lines else [question]\n",
    "\n",
    "def search_once(q: str, k: int = 6) -> List[Document]:\n",
    "    # uses the parent/child retriever (LangChain 1.0 uses invoke() instead of get_relevant_documents())\n",
    "    return retriever.invoke(q)[:k]\n",
    "\n",
    "def reciprocal_rank_fusion(list_of_results: List[List[Document]], k: int = 60) -> List[Document]:\n",
    "    # RRF: score = sum(1/(k + rank(d)))\n",
    "    scores = {}\n",
    "    for results in list_of_results:\n",
    "        seen = {}\n",
    "        for rank, d in enumerate(results, start=1):\n",
    "            key = (d.metadata.get(\"source\"), d.metadata.get(\"idx\"), d.page_content[:50])\n",
    "            if key in seen: \n",
    "                continue\n",
    "            seen[key] = True\n",
    "            scores[key] = scores.get(key, 0) + 1.0 / (k + rank)\n",
    "    ranked = sorted(scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    # reconstruct Documents from keys; here we return the first existing results\n",
    "    flat = list(itertools.chain.from_iterable(list_of_results))\n",
    "    out = []\n",
    "    for key, _ in ranked:\n",
    "        for d in flat:\n",
    "            if (d.metadata.get(\"source\"), d.metadata.get(\"idx\"), d.page_content[:50]) == key:\n",
    "                out.append(d)\n",
    "                break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Advanced RAG Pipeline\n",
    "\n",
    "Combine all advanced techniques into a single, powerful RAG pipeline that demonstrates the full potential of advanced retrieval methods.\n",
    "\n",
    "**What this section does:**\n",
    "- `rag_advanced_pipeline()` integrates:\n",
    "  1. **Query Expansion**: Generates 4 reformulations of the query\n",
    "  2. **Multi-query Retrieval**: Searches with original + expanded queries\n",
    "  3. **RAG-Fusion**: Merges results using Reciprocal Rank Fusion\n",
    "  4. **Reranking**: LLM-based reranking of top candidates\n",
    "  5. **Generation**: Final answer generation with reranked context\n",
    "\n",
    "**Pipeline Flow:**\n",
    "```\n",
    "Query â†’ Query Expansion â†’ Multi-query Retrieval â†’ RRF Fusion â†’ LLM Reranking â†’ Generation\n",
    "```\n",
    "\n",
    "This demonstrates a production-ready advanced RAG system that combines multiple techniques for optimal performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Œ Initial Question:\n",
      "Explain who are each character of these stories.\n",
      "============================================================\n",
      "\n",
      "ðŸ’¡ Query Expansions (4):\n",
      "  1. Can you describe the characters in these stories?\n",
      "  2. Who are the characters in each of these narratives?\n",
      "  3. Please provide an overview of the characters from these tales.\n",
      "  4. What can you tell me about the characters in these stories?\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ” Single Retrieval Results (5 queries):\n",
      "  Query 1: 2 results\n",
      "  Query 2: 2 results\n",
      "  Query 3: 2 results\n",
      "  Query 4: 1 results\n",
      "  Query 5: 2 results\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ”— Fused Results: 2 passages (showing top 3):\n",
      "  - Jenny bought a toaster from a mysterious yard sale, not realizing it was a time machine. Every time she toasted bread, it popped up with tiny messages...\n",
      "  - In the small town of Noodleton, spaghetti rained from the sky for no reason at all. Mayor Luigi tried to stop it with umbrellas, but the noodles were ...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ† Reranked Top 2 Results:\n",
      "  1. In the small town of Noodleton, spaghetti rained from the sky for no reason at all. Mayor Luigi tried to stop it with umbrellas, but the noodles were ...\n",
      "  2. Jenny bought a toaster from a mysterious yard sale, not realizing it was a time machine. Every time she toasted bread, it popped up with tiny messages...\n",
      "============================================================\n",
      "\n",
      "âœ… Final Answer:\n",
      "**Characters:**\n",
      "\n",
      "1. **Mayor Luigi** - The leader of Noodleton who attempts to manage the chaos caused by the spaghetti rain but ultimately embraces the fun of the situation.\n",
      "\n",
      "2. **Max** - An ambitious dog who turns the falling spaghetti into a game, becoming a town hero and leading the community in celebrating the phenomenon with annual noodle festivals.\n",
      "\n",
      "3. **Jenny** - A curious individual who unknowingly buys a time-traveling toaster, which provides her with future messages and allows her to send warnings back in time.\n",
      "\n",
      "4. **Dancing Squirrels** - A whimsical group that disrupts Jenny's life with their unexpected flash mob, adding humor and chaos to her daily routine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"**Characters:**\\n\\n1. **Mayor Luigi** - The leader of Noodleton who attempts to manage the chaos caused by the spaghetti rain but ultimately embraces the fun of the situation.\\n\\n2. **Max** - An ambitious dog who turns the falling spaghetti into a game, becoming a town hero and leading the community in celebrating the phenomenon with annual noodle festivals.\\n\\n3. **Jenny** - A curious individual who unknowingly buys a time-traveling toaster, which provides her with future messages and allows her to send warnings back in time.\\n\\n4. **Dancing Squirrels** - A whimsical group that disrupts Jenny's life with their unexpected flash mob, adding humor and chaos to her daily routine.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_advanced_pipeline(question: str) -> str:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸ“Œ Initial Question:\\n{question}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Expansions\n",
    "    exps = query_expansions(question, n=4)\n",
    "    print(f\"ðŸ’¡ Query Expansions ({len(exps)}):\")\n",
    "    for i, exp in enumerate(exps, 1):\n",
    "        print(f\"  {i}. {exp}\")\n",
    "    print(\"-\"*60 + \"\\n\")\n",
    "\n",
    "    # Retrieval\n",
    "    all_results = [search_once(q, k=6) for q in [question] + exps]\n",
    "    print(f\"ðŸ” Single Retrieval Results ({len(all_results)} queries):\")\n",
    "    for i, results in enumerate(all_results, 1):\n",
    "        print(f\"  Query {i}: {len(results)} results\")\n",
    "    print(\"-\"*60 + \"\\n\")\n",
    "\n",
    "    # Fusion\n",
    "    fused = reciprocal_rank_fusion(all_results, k=60)\n",
    "    print(f\"ðŸ”— Fused Results: {len(fused)} passages (showing top 3):\")\n",
    "    for d in fused[:3]:\n",
    "        snippet = d.page_content[:150].replace(\"\\n\", \" \") + (\"...\" if len(d.page_content) > 150 else \"\")\n",
    "        print(f\"  - {snippet}\")\n",
    "    print(\"-\"*60 + \"\\n\")\n",
    "\n",
    "    # Reranking\n",
    "    reranked = rerank_llm(fused[:8], question, k=3)\n",
    "    print(f\"ðŸ† Reranked Top {len(reranked)} Results:\")\n",
    "    for i, d in enumerate(reranked, 1):\n",
    "        snippet = d.page_content[:150].replace(\"\\n\", \" \") + (\"...\" if len(d.page_content) > 150 else \"\")\n",
    "        print(f\"  {i}. {snippet}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Final generation\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an assistant who responds concisely by explicitly citing the key ideas from the provided PASSAGES.\"),\n",
    "        (\"human\", \"QUESTION:\\n{q}\\n\\nPASSAGES:\\n{ctx}\\n\\nAnswer:\")\n",
    "    ])\n",
    "    ctx = \"\\n---\\n\".join(d.page_content for d in reranked)\n",
    "    chain = prompt | llm\n",
    "    resp = chain.invoke({\"q\": question, \"ctx\": ctx[:4000]})\n",
    "\n",
    "    print(\"âœ… Final Answer:\")\n",
    "    print(resp.content)\n",
    "    return resp.content\n",
    "\n",
    "\n",
    "# Example call\n",
    "rag_advanced_pipeline(\"Explain who are each character of these stories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Query Planning (Plan-and-Execute)\n",
    "\n",
    "Implement a **query planning** system that decomposes complex questions into atomic sub-questions and executes them sequentially.\n",
    "**Query Planning** (plan-and-execute pattern) enables sophisticated multi-step reasoning by breaking down complex questions into manageable atomic sub-tasks.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "**Planning Phase**\n",
    "- LLM decomposes the question into 2-5 atomic steps\n",
    "- Each step includes:\n",
    "  - A description of what needs to be done\n",
    "  - A specific query to execute\n",
    "  - An assigned tool (retrieval, calculation, none, etc.)\n",
    "- Enables multi-step reasoning and tool orchestration\n",
    "\n",
    "**Execution Phase**\n",
    "- Each step is executed independently\n",
    "- Results are collected as \"notes\" or intermediate findings\n",
    "- Tools can include:\n",
    "  - **Retrieval**: Search the knowledge base\n",
    "  - **Calculation**: Perform computations (use with caution in production!)\n",
    "  - **None**: Direct reasoning without tools\n",
    "\n",
    "**Synthesis Phase**\n",
    "- Final LLM call combines all notes into a coherent answer\n",
    "- Maintains logical flow and structure\n",
    "- Integrates information from multiple sources\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "âœ… **Best for complex questions requiring:**\n",
    "- Multiple retrieval queries (e.g., \"Compare X and Y\")\n",
    "- Sequential reasoning (e.g., \"First find A, then use A to find B\")\n",
    "- Integration of different information sources\n",
    "- Tool usage beyond simple retrieval\n",
    "- Multi-part analytical questions\n",
    "\n",
    "âŒ **Not necessary for:**\n",
    "- Simple, single-answer questions\n",
    "- Direct fact retrieval\n",
    "- Questions answerable with one retrieval call\n",
    "\n",
    "\n",
    "**What this section does:**\n",
    "\n",
    "### Planning Phase\n",
    "- `simple_planner()`: Uses an LLM to break down complex questions into 2-5 atomic steps\n",
    "- Each step includes:\n",
    "  - **step**: Description of what needs to be done\n",
    "  - **query**: Specific query to execute\n",
    "  - **tool**: Tool to use (retrieval, calculation, or none)\n",
    "\n",
    "### Execution Phase\n",
    "- `execute_plan()`: Executes each step in the plan:\n",
    "  - **Retrieval tool**: Searches the knowledge base and reranks results\n",
    "  - **Calculation tool**: Performs computations (âš ï¸ uses `eval()` - not for production!)\n",
    "  - **None tool**: Direct reasoning without tools\n",
    "- Collects results as \"notes\" from each step\n",
    "\n",
    "### Synthesis Phase\n",
    "- Final LLM call synthesizes all notes into a coherent, structured answer\n",
    "- Maintains logical flow and integrates information from multiple sources\n",
    "\n",
    "**Use Case:** Best for complex, multi-part questions that require sequential reasoning or multiple information sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN: [{'step': 'Identify the main character', 'query': 'Who is the main character in the story?', 'tool': 'retrieval'}, {'step': 'List personality traits', 'query': 'What are the personality traits of the main character?', 'tool': 'retrieval'}, {'step': 'Identify another character for comparison', 'query': 'Who is the other character to compare with?', 'tool': 'retrieval'}, {'step': 'List personality traits of the other character', 'query': 'What are the personality traits of the other character?', 'tool': 'retrieval'}, {'step': 'Compare the personality traits', 'query': 'How do the personality traits of the two characters compare?', 'tool': 'none'}]\n",
      "\n",
      "ANSWER:\n",
      " **Main Character: Jenny**\n",
      "\n",
      "- **Personality Traits:**\n",
      "  - Adventurous: Jenny embraces the unexpected, as seen when she buys a mysterious toaster.\n",
      "  - Curious: She explores the capabilities of the toaster, discovering it can send messages from the future.\n",
      "  - Playful: Jenny turns her breakfast routine into an adventure, enjoying the whimsical messages from the toast.\n",
      "\n",
      "**Other Character: Mr. Jonas**\n",
      "\n",
      "- **Personality Traits:**\n",
      "  - Laid-back: Mr. Jonas enjoys napping and initially prefers a relaxed lifestyle.\n",
      "  - Creative: He innovates by turning a mishap into a motivational strategy at work.\n",
      "  - Charismatic: His charm wins over the office staff, leading to unexpected success in his role.\n",
      "\n",
      "**Comparison of Personality Traits:**\n",
      "\n",
      "- **Adventurous vs. Laid-back:** Jenny's adventurous spirit contrasts with Mr. Jonas's more relaxed demeanor. While Jenny actively seeks out new experiences, Mr. Jonas is content with a slower pace until he is inspired to take action.\n",
      "  \n",
      "- **Curious vs. Creative:** Jenny's curiosity drives her to explore the toaster's capabilities, while Mr. Jonas's creativity allows him to turn a mistake into a successful office initiative. Both characters exhibit a willingness to embrace the unusual, but they do so in different ways.\n",
      "\n",
      "- **Playful vs. Charismatic:** Jenny's playful nature is evident in her interactions with the toaster and the whimsical situations it creates. In contrast, Mr. Jonas's charisma helps him connect with others and implement changes that enhance workplace culture.\n",
      "\n",
      "Overall, both characters exhibit unique traits that lead them to navigate their respective adventures in imaginative and humorous ways, highlighting the value of curiosity and creativity in their lives.\n"
     ]
    }
   ],
   "source": [
    "# 6) Query Planning (minimal plan-and-execute)\n",
    "plan_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a planner. Decompose the QUESTION into 2â€“5 atomic steps in JSON format: \"\n",
    "               \"[{{\\\"step\\\": \\\"...\\\", \\\"query\\\": \\\"...\\\", \\\"tool\\\": \\\"retrieval|calc|none\\\"}}]\"),\n",
    "    (\"human\", \"QUESTION: {q}\")\n",
    "])\n",
    "\n",
    "def simple_planner(question: str) -> List[Dict[str, Any]]:\n",
    "    # Use LCEL pattern for LangChain 1.0 compatibility\n",
    "    chain = plan_prompt | llm\n",
    "    resp = chain.invoke({\"q\": question})\n",
    "    try:\n",
    "        plan = json.loads(resp.content)\n",
    "        if isinstance(plan, list):\n",
    "            return plan[:5]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return [{\"step\": \"General search\", \"query\": question, \"tool\": \"retrieval\"}]\n",
    "\n",
    "def execute_plan(plan: List[Dict[str, Any]]) -> str:\n",
    "    notes = []\n",
    "    for p in plan:\n",
    "        tool = p.get(\"tool\", \"retrieval\")\n",
    "        q = p.get(\"query\", \"\")\n",
    "        if tool == \"retrieval\":\n",
    "            docs = retriever.invoke(q)[:6]  # LangChain 1.0 uses invoke() instead of get_relevant_documents()\n",
    "            docs = rerank_llm(docs, q, k=2)\n",
    "            notes.append(\"\\n\".join(d.page_content for d in docs))\n",
    "        elif tool == \"calc\":\n",
    "            # placeholder for external calculation (e.g., Python)\n",
    "            try:\n",
    "                res = eval(q)  # âš ï¸ DO NOT do this in production (security), controlled demo here\n",
    "            except Exception:\n",
    "                res = f\"calc_failed({q})\"\n",
    "            notes.append(str(res))\n",
    "        else:\n",
    "            notes.append(f\"(no tool) {q}\")\n",
    "    # final synthesis\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Synthesize the NOTES below into a clear and structured answer.\"),\n",
    "        (\"human\", \"QUESTION: {q}\\nNOTES:\\n{notes}\\nAnswer:\")\n",
    "    ])\n",
    "    # Use LCEL pattern for LangChain 1.0 compatibility\n",
    "    chain = prompt | llm\n",
    "    resp = chain.invoke({\n",
    "        \"q\": \"; \".join([p.get(\"query\",\"\") for p in plan]), \n",
    "        \"notes\": \"\\n---\\n\".join(notes)[:5000]\n",
    "    })\n",
    "    return resp.content\n",
    "\n",
    "question = \"Compare main caracter's personality\"\n",
    "plan = simple_planner(question)\n",
    "print(\"PLAN:\", plan)\n",
    "print(\"\\nANSWER:\\n\", execute_plan(plan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Evaluation with LLM-as-a-Judge\n",
    "\n",
    "Implement evaluation using an **LLM-as-a-Judge** approach to assess answer quality.\n",
    "\n",
    "**âœ… Advantages:**\n",
    "- Flexible evaluation criteria (can assess clarity, completeness, nuance)\n",
    "- No need for labeled datasets\n",
    "- Can evaluate open-ended questions\n",
    "- Understands context and semantic similarity\n",
    "\n",
    "**âŒ Limitations:**\n",
    "- Non-deterministic (same answer may get different scores)\n",
    "- Potential bias based on judge model\n",
    "- Computationally expensive\n",
    "- May not align with human judgment perfectly\n",
    "\n",
    "\n",
    "**What this section does:**\n",
    "- `judge_answer()`: Evaluates answer quality using an LLM judge\n",
    "- **Criteria evaluated:**\n",
    "  - âœ… **Factual Accuracy**: Alignment with retrieved passages\n",
    "  - âœ… **Completeness**: Coverage of important aspects\n",
    "  - âœ… **Clarity**: How well the answer is structured and explained\n",
    "- Returns a score (0-100) and detailed feedback\n",
    "\n",
    "**Example Usage:**\n",
    "- Generates an answer using the advanced pipeline\n",
    "- Retrieves relevant context documents\n",
    "- Judges the answer quality against the context\n",
    "\n",
    "**Note:** This is a demonstration of LLM-based evaluation. For production, consider using RAGAS framework or human evaluation for more reliable metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ“Œ Initial Question:\n",
      "Is Jonas a black cat?\n",
      "============================================================\n",
      "\n",
      "ðŸ’¡ Query Expansions (4):\n",
      "  1. Is Jonas a cat of the black variety?\n",
      "  2. Is Jonas a feline that is black in color?\n",
      "  3. Is Jonas classified as a black cat?\n",
      "  4. Does Jonas belong to the black cat category?\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ” Single Retrieval Results (5 queries):\n",
      "  Query 1: 1 results\n",
      "  Query 2: 1 results\n",
      "  Query 3: 1 results\n",
      "  Query 4: 1 results\n",
      "  Query 5: 1 results\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ”— Fused Results: 1 passages (showing top 3):\n",
      "  - Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local office. Unfortunately, during th...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ† Reranked Top 1 Results:\n",
      "  1. Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local office. Unfortunately, during th...\n",
      "============================================================\n",
      "\n",
      "âœ… Final Answer:\n",
      "No, Jonas is not a black cat; he is described as a fluffy orange cat.\n",
      "Score: 95\n",
      "\n",
      "Remarks:\n",
      "1. The answer is factually accurate and directly addresses the question by confirming that Jonas is not a black cat but a fluffy orange cat, which aligns perfectly with the information provided in the passages.\n",
      "2. The response is clear and concise, effectively summarizing the relevant details without unnecessary elaboration, though it could have briefly mentioned the context of Jonas's character to enhance completeness.\n"
     ]
    }
   ],
   "source": [
    "judge = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "judge_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You evaluate the answer. Criteria: factual accuracy vs PASSAGES, completeness, clarity. Give a score 0-100 and 2 remarks.\"),\n",
    "    (\"human\", \"QUESTION: {q}\\nANSWER: {ans}\\nPASSAGES: {ctx}\\nScore and feedback:\")\n",
    "])\n",
    "\n",
    "def judge_answer(question: str, answer: str, ctx_docs: List[Document]) -> str:\n",
    "    ctx = \"\\n---\\n\".join(d.page_content for d in ctx_docs[:5])\n",
    "    # Use LCEL pattern for LangChain 1.0 compatibility\n",
    "    chain = judge_prompt | judge\n",
    "    resp = chain.invoke({\"q\": question, \"ans\": answer, \"ctx\": ctx[:4000]})\n",
    "    return resp.content\n",
    "\n",
    "q = \"Is Jonas a black cat?\"\n",
    "ans = rag_advanced_pipeline(q)\n",
    "ctx_docs = retriever.invoke(q)[:5]  # LangChain 1.0 uses invoke() instead of get_relevant_documents()\n",
    "print(judge_answer(q, ans, ctx_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
