{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**This notebook provides a reference solution using LangChain.**\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "- Understand the core concepts of RAG (Retrieval-Augmented Generation)\n",
    "- Implement a basic RAG pipeline using LangChain\n",
    "- Create vector embeddings and perform semantic search\n",
    "- Build a question-answering system with context retrieval\n",
    "- Visualize and analyze the RAG pipeline components\n",
    "\n",
    "## üéØ What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that combines:\n",
    "1. **Retrieval**: Finding relevant information from a knowledge base\n",
    "2. **Augmentation**: Injecting this context into the prompt\n",
    "3. **Generation**: Using a language model to generate answers based on the retrieved context\n",
    "\n",
    "This approach improves factual accuracy and reduces hallucinations by grounding the model's responses in actual documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility fix for langchain_google_genai with newer langchain_core\n",
    "# Run this cell FIRST before importing langchain_google_genai\n",
    "import sys\n",
    "import pydantic\n",
    "import warnings\n",
    "\n",
    "# Create a compatibility shim for pydantic_v1\n",
    "# This provides Pydantic v1 compatibility for langchain_google_genai\n",
    "class PydanticV1Compat:\n",
    "    \"\"\"Compatibility shim for pydantic_v1 imports\"\"\"\n",
    "    def __getattr__(self, name):\n",
    "        # Handle root_validator specially for Pydantic v2 compatibility\n",
    "        if name == 'root_validator':\n",
    "            # Return a decorator that works with Pydantic v2\n",
    "            def root_validator(*args, **kwargs):\n",
    "                # Suppress warnings for deprecated root_validator\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    # Try to use model_validator if available (Pydantic v2)\n",
    "                    if hasattr(pydantic, 'model_validator'):\n",
    "                        return pydantic.model_validator(mode='before', *args, **kwargs)\n",
    "                    # Fallback to field_validator or other v2 validators\n",
    "                    return lambda f: f\n",
    "            return root_validator\n",
    "        return getattr(pydantic, name)\n",
    "\n",
    "# Patch langchain_core.pydantic_v1 if it doesn't exist\n",
    "try:\n",
    "    import langchain_core\n",
    "    if not hasattr(langchain_core, 'pydantic_v1'):\n",
    "        langchain_core.pydantic_v1 = PydanticV1Compat()\n",
    "        sys.modules['langchain_core.pydantic_v1'] = langchain_core.pydantic_v1\n",
    "        print(\"‚úÖ Compatibility fix applied for langchain_google_genai\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Warning: Could not apply compatibility fix: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "First, let's install the required libraries:\n",
    "- `langchain`: Framework for building LLM applications\n",
    "- `langchain-openai`: OpenAI integration for LangChain\n",
    "- `chromadb`: Vector database (alternative to Qdrant)\n",
    "- `qdrant-cpu`: Facebook AI Similarity Search for efficient vector similarity search\n",
    "- `tiktoken`: Tokenizer for OpenAI models\n",
    "- `pypdf`: PDF processing library (for future exercises)\n",
    "- `matplotlib`, `seaborn`: For data visualization\n",
    "- `numpy`: For numerical operations\n",
    "- `scikit-learn`: For PCA and similarity calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import langchain_community\n",
    "print(langchain.__version__)\n",
    "print(langchain_community.__version__)\n",
    "!uv pip freeze | grep qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîê API Key Setup\n",
    "\n",
    "Make sure you have your OpenAI API key set in your environment variables. You can set it by:\n",
    "- Exporting it in your terminal: `export OPENAI_API_KEY=\"your-key-here\"`\n",
    "- Or using a `.env` file with `python-dotenv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Determine which backend to use in the .env\n",
    "USE_OLLAMA = os.environ.get(\"USE_OLLAMA\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "USE_GEMINI = os.environ.get(\"USE_GEMINI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    # Utiliser Ollama\n",
    "    from langchain_community.chat_models import ChatOllama\n",
    "    # Assurez-vous que le serveur Ollama est en cours d'ex√©cution\n",
    "    # et que le mod√®le 'mistral' est t√©l√©charg√©.\n",
    "    llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "    print(\"‚öôÔ∏è Utilisation du backend Ollama (mistral)\")\n",
    "elif USE_GEMINI:\n",
    "    # Utiliser Gemini\n",
    "    # Assurez-vous que la variable d'environnement GOOGLE_API_KEY est d√©finie dans le .env\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    print(\"‚öôÔ∏è Utilisation du backend Gemini (gemini-2.5-flash)\")\n",
    "else:\n",
    "    # Utiliser OpenAI par d√©faut\n",
    "    # Assurez-vous que la variable d'environnement OPENAI_API_KEY est d√©finie dans le .env\n",
    "    from langchain_openai.chat_models import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    print(\"‚öôÔ∏è Utilisation du backend OpenAI (gpt-4o-mini)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Building a Basic RAG Pipeline\n",
    "\n",
    "Let's build a complete RAG pipeline step by step. The pipeline consists of:\n",
    "\n",
    "1. **Document Loading**: Prepare your documents\n",
    "2. **Text Splitting**: Break documents into smaller chunks\n",
    "3. **Embedding**: Convert text chunks into vector embeddings\n",
    "4. **Vector Storage**: Store embeddings in a vector database\n",
    "5. **Retrieval**: Find relevant chunks for a query\n",
    "6. **Generation**: Generate answers using retrieved context\n",
    "\n",
    "Let's implement this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Modern LangChain imports (compatible with LangChain 0.1+)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Step 1: Define our sample documents\n",
    "documents_raw = [\n",
    "    \"\"\"Mr. Jonas, a fluffy orange cat, decided he was tired of just napping all day. He applied for a promotion at the local office. Unfortunately, during the interview, he fell asleep on the keyboard, accidentally sending a company-wide email that said, 'Meow if you approve!' Surprisingly, the staff loved it, thinking it was a new motivational strategy. Mr. Jonas got the promotion, now supervising the office naps. Soon, he implemented 'cat breaks' every hour, where employees were encouraged to stretch, purr softly, or chase imaginary laser pointers. Productivity oddly skyrocketed, and the office became famous for its feline-inspired efficiency seminars. Mr. Jonas even started a newsletter titled 'The Purr-fect Manager,' sharing tips on balancing work and naps.\"\"\",\n",
    "    \"\"\"Jenny bought a toaster from a mysterious yard sale, not realizing it was a time machine. Every time she toasted bread, it popped up with tiny messages from the future. One morning, the toast said, 'Beware the dancing squirrels at 3 PM!' She ignored it, and sure enough, a flash mob of squirrels waltzed through her garden, all wearing tiny top hats. Jenny now always eats her breakfast with a helmet. Eventually, she discovered a secret lever inside the toaster that allowed her to send messages back in time. She used it to warn her past self about spilled coffee, missed deadlines, and unexpected squirrel invasions. Breakfast became an adventure, with toast revealing mysteries of the universe and sometimes giving advice on what socks to wear for luck.\"\"\",\n",
    "    \"\"\"In the small town of Noodleton, spaghetti rained from the sky for no reason at all. Mayor Luigi tried to stop it with umbrellas, but the noodles were relentless. One ambitious dog named Max decided to chase the falling spaghetti like a game. Soon, the whole town joined, slipping, sliding, and twirling in a chaotic noodle dance-off. By evening, everyone agreed: it was the tastiest disaster in history. Over the next few days, chefs experimented with sky-spaghetti recipes, inventing noodle ice cream, spaghetti smoothies, and pasta hats. Scientists tried to figure out the phenomenon but concluded it was 'spaghetti magic.' Max became the town hero, leading annual 'noodle festivals,' and Noodleton transformed into the world's first pasta-themed tourist destination.\"\"\"\n",
    "]\n",
    "\n",
    "# Step 2: Convert raw texts to Document objects\n",
    "docs = [Document(page_content=txt, metadata={\"source\": \"local\", \"idx\": i})\n",
    "        for i, txt in enumerate(documents_raw)]\n",
    "\n",
    "print(f\"‚úÖ Created {len(docs)} documents\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"  Content: {doc.page_content[:100]}...\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìè Text Splitting (Chunking)\n",
    "\n",
    "**Why chunk documents?**\n",
    "- Language models have token limits (context windows)\n",
    "- Smaller chunks are easier to retrieve precisely\n",
    "- Overlapping chunks preserve context at boundaries\n",
    "\n",
    "**Parameters:**\n",
    "- `chunk_size`: Maximum size of each chunk (in characters)\n",
    "- `chunk_overlap`: Overlap between chunks to preserve context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split documents into chunks\n",
    "# RecursiveCharacterTextSplitter tries to split on paragraphs, sentences, then words\n",
    "# to preserve semantic meaning as much as possible\n",
    "CHUNK_SIZE = 200\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,      # Maximum characters per chunk\n",
    "    chunk_overlap=50     # Overlap between chunks (helps preserve context)\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split {len(docs)} documents into {len(chunks)} chunks\")\n",
    "print(\"\\nChunk examples:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content}\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if `chunk_size` is too small? Too large?\n",
    "\n",
    "- **If `chunk_size` is too small:**  \n",
    "  - **Pros:** Each chunk is very focused, minimizing irrelevant information.  \n",
    "  - **Cons:** Important context can be lost because sentences/paragraphs are split. Retrieval may return partial or incoherent answers.  \n",
    "  - **Performance:** Number of chunks increases ‚Üí more memory/storage usage and slower embedding/search.\n",
    "\n",
    "- **If `chunk_size` is too large:**  \n",
    "  - **Pros:** More context is preserved in each chunk, helping the model produce complete answers.  \n",
    "  - **Cons:** Irrelevant text may be included, which can confuse retrieval.  \n",
    "  - **Performance:** Fewer chunks ‚Üí faster embedding, but long chunks may exceed the model‚Äôs token limit during retrieval/generation.\n",
    "\n",
    "**Rule of thumb:** Choose chunks that fit comfortably within your LLM‚Äôs context window (e.g., 500‚Äì1000 tokens for GPT-3.5/GPT-4 small tasks).\n",
    "\n",
    "#### How does `chunk_overlap` affect retrieval quality?\n",
    "\n",
    "- `chunk_overlap` defines how many tokens/characters are shared between consecutive chunks.  \n",
    "\n",
    "- **Higher overlap:**  \n",
    "  - **Pros:** Preserves context across chunk boundaries ‚Üí improves retrieval for multi-sentence queries.  \n",
    "  - **Cons:** More redundant chunks ‚Üí slightly higher storage and embedding costs.  \n",
    "\n",
    "- **Lower overlap:**  \n",
    "  - **Pros:** Less redundancy ‚Üí faster processing and lower storage.  \n",
    "  - **Cons:** May lose context at chunk boundaries ‚Üí retrieval can miss important information.  \n",
    "\n",
    "**Tip:** Typical overlap is 50‚Äì100 tokens (~20‚Äì30% of chunk size).\n",
    "\n",
    "#### What's the optimal chunk size for your use case?\n",
    "\n",
    "- **Depends on:**  \n",
    "  - Document type (dense technical PDFs vs. short news articles)  \n",
    "  - Query complexity (simple fact lookup vs. multi-sentence reasoning)  \n",
    "  - LLM context window (e.g., GPT-4 has 8k‚Äì32k tokens)  \n",
    "\n",
    "**Practical guideline:**  \n",
    "- Start with:  \n",
    "  - `chunk_size = 500‚Äì1000 tokens`  \n",
    "  - `chunk_overlap = 50‚Äì100 tokens`  \n",
    "\n",
    "- Test retrieval quality:  \n",
    "  - If answers are incomplete ‚Üí increase chunk size or overlap.  \n",
    "  - If answers include too much irrelevant info ‚Üí decrease chunk size.  \n",
    "\n",
    "üí° **Extra tip:** Visualize a few chunks to ensure they contain coherent, self-contained information. This is often the easiest way to find the ‚Äúsweet spot.‚Äù\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal RAG Implementation (Without LangChain Wrappers)\n",
    "\n",
    "Below is a compact reference implementation showing how to:\n",
    "\n",
    "- Call the embeddings API directly (supports OpenAI and Gemini based on `USE_GEMINI` environment variable)\n",
    "- Build an in-memory index\n",
    "- Perform cosine similarity search\n",
    "- Generate answers using the configured LLM backend (OpenAI or Gemini)\n",
    "\n",
    "This helps you understand what happens under the hood of LangChain. The implementation automatically uses the backend specified by your environment variables (same as the rest of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Determine which backend to use\n",
    "USE_OLLAMA = os.environ.get(\"USE_OLLAMA\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "USE_GEMINI = os.environ.get(\"USE_GEMINI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "def embed_texts(texts):\n",
    "    \"\"\"Convert texts to embeddings using the configured backend\"\"\"\n",
    "    if USE_GEMINI:\n",
    "        # Use Google Gemini embeddings\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "        # Embed multiple texts\n",
    "        embeddings_list = []\n",
    "        for text in texts:\n",
    "            result = genai.embed_content(\n",
    "                model=\"models/embedding-001\",\n",
    "                content=text,\n",
    "                task_type=\"retrieval_document\"\n",
    "            )\n",
    "            embeddings_list.append(result['embedding'])\n",
    "        return embeddings_list\n",
    "    else:\n",
    "        # Use OpenAI embeddings (default)\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        resp = client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\", \n",
    "            input=texts\n",
    "        )\n",
    "        return [d.embedding for d in resp.data]\n",
    "\n",
    "def generate_answer(context, query):\n",
    "    \"\"\"Generate answer using the configured LLM backend\"\"\"\n",
    "    if USE_GEMINI:\n",
    "        # Use Google Gemini\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "        model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful assistant. Use the provided context chunks to comprehensively answer the user's query. Do not use external knowledge. If the context does not contain enough information, state that clearly.\n",
    "\n",
    "{context}\n",
    "\n",
    "User Query: {query}\"\"\"\n",
    "        \n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    else:\n",
    "        # Use OpenAI (default)\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the provided context chunks to comprehensively answer the user's query. Do not use external knowledge. If the context does not contain enough information, state that clearly.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{context}\\n\\nUser Query: {query}\"}\n",
    "            ]\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# Index in memory\n",
    "raw_texts = [c.page_content for c in chunks]  # Reuse 'chunks' from above\n",
    "vecs = embed_texts(raw_texts)\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-8) * (np.linalg.norm(b) + 1e-8)))\n",
    "\n",
    "def search(query, k=3):\n",
    "    \"\"\"Search for k most similar chunks to the query\"\"\"\n",
    "    # Get query embedding\n",
    "    qv = embed_texts([query])[0]\n",
    "    \n",
    "    # Calculate similarities\n",
    "    scored = [(cos_sim(qv, v), t) for v, t in zip(vecs, raw_texts)]\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Return top k\n",
    "    return [(score, text) for score, text in scored[:k]]\n",
    "\n",
    "# Quick demo\n",
    "results = search(\"Explain Jonas's story\", k=2)\n",
    "for score, text in results:\n",
    "    print(f\"Similarity: {score:.3f}\")\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Prepare context for the LLM\n",
    "context = \"\"\n",
    "for score, text in results:\n",
    "    context += f\"***Similarity Score: {score:.3f}***\\nCONTEXT CHUNK:\\n{text}\\n---\\n\"\n",
    "\n",
    "# Final LLM call to generate a complete answer based on the retrieved context\n",
    "final_answer = generate_answer(context, \"Explain Jonas's story\")\n",
    "\n",
    "# Print the final generated answer\n",
    "print(\"\\n--- FINAL ANSWER GENERATED BY LLM ---\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ Creating Embeddings and Vector Store\n",
    "\n",
    "**Embeddings** are numerical representations of text that capture semantic meaning.\n",
    "Similar texts have similar embeddings, allowing us to find relevant content through\n",
    "vector similarity search.\n",
    "\n",
    "**Qdrant** is a library for efficient similarity search and clustering of dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "# Step 4: Create embeddings based on backend selection\n",
    "# Check which backend to use (from the setup cell)\n",
    "USE_OLLAMA = os.environ.get(\"USE_OLLAMA\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "USE_GEMINI = os.environ.get(\"USE_GEMINI\", \"\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "if USE_GEMINI:\n",
    "    # Use Google Gemini embeddings\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    embedding_dim = 768  # Gemini embedding dimension\n",
    "    print(\"‚öôÔ∏è Using Google Gemini embeddings\")\n",
    "else:\n",
    "    # Use OpenAI embeddings (default)\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    embedding_dim = 1536  # OpenAI text-embedding-3-small dimension\n",
    "    print(\"‚öôÔ∏è Using OpenAI embeddings\")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create vector store from chunks\n",
    "# Qdrant stores the embeddings and enables fast similarity search\n",
    "# Use location=\":memory:\" for in-memory storage\n",
    "\n",
    "# Create Qdrant client\n",
    "client = QdrantClient(location=\":memory:\")\n",
    "\n",
    "# Delete collection if it exists to start fresh\n",
    "try:\n",
    "    client.delete_collection('docs')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create collection with proper configuration\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "client.create_collection(\n",
    "    collection_name='docs',\n",
    "    vectors_config=VectorParams(\n",
    "        size=embedding_dim,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create vector store using QdrantVectorStore (recommended, non-deprecated)\n",
    "vectordb = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name='docs',\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Add documents to the vector store\n",
    "vectordb.add_documents(chunks)\n",
    "\n",
    "# Get the number of documents (Qdrant doesn't support len() directly)\n",
    "# We can check by getting all documents or using the client\n",
    "print(f\"‚úÖ Vector store created successfully\")\n",
    "print(f\"   Number of chunks: {len(chunks)}\")\n",
    "print(f\"   Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Setting Up the Retriever and QA Chain\n",
    "\n",
    "A **retriever** finds the most relevant chunks for a query.\n",
    "The **QA chain** combines retrieval with generation to answer questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create a retriever\n",
    "# The retriever will find the k most similar chunks for each query\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 chunks\n",
    "\n",
    "# Step 7: Create the QA chain using modern LangChain patterns\n",
    "# Use the llm from the setup cell (already configured for Gemini/OpenAI/Ollama)\n",
    "\n",
    "# Create a prompt template for the QA chain\n",
    "system_prompt = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Keep the answer concise and focused on the information provided in the context.\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the document chain (handles the \"stuff\" approach - concatenates all docs)\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Create the retrieval chain (combines retriever + document chain)\n",
    "qa = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "print(\"‚úÖ RAG pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Testing the RAG System\n",
    "\n",
    "Let's ask a question and see how the system retrieves relevant context and generates an answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "questions = [\n",
    "    \"Who is Mr. Jonas?\",\n",
    "    \"What Jenny's toast said?\",\n",
    "    \"Which object the Mayor Luigi used to try to stop the Spaghetti rain?\",\n",
    "]\n",
    "for question in questions:\n",
    "    answer = qa.invoke({\"input\": question})\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"QUESTION:\")\n",
    "    print(question)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ANSWER:\")\n",
    "    print(answer[\"answer\"])\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualizing the RAG Pipeline\n",
    "\n",
    "Let's create some visualizations to better understand what's happening in the RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Visualization 1: Document Chunking Analysis\n",
    "\n",
    "Let's visualize how documents are split into chunks and their sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunk sizes\n",
    "chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "chunk_indices = list(range(len(chunks)))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of chunk sizes\n",
    "ax[0].bar(chunk_indices, chunk_sizes, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "ax[0].axhline(y=CHUNK_SIZE, color='r', linestyle='--', label='Max chunk size (CHUNK_SIZE)')\n",
    "ax[0].set_xlabel('Chunk Index')\n",
    "ax[0].set_ylabel('Chunk Size (characters)')\n",
    "ax[0].set_title('Chunk Sizes Distribution')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax[1].boxplot(chunk_sizes, vert=True, patch_artist=True,\n",
    "              boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "ax[1].set_ylabel('Chunk Size (characters)')\n",
    "ax[1].set_title('Chunk Size Statistics')\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(f\"  Total chunks: {len(chunks)}\")\n",
    "print(f\"  Average size: {np.mean(chunk_sizes):.1f} characters\")\n",
    "print(f\"  Min size: {min(chunk_sizes)} characters\")\n",
    "print(f\"  Max size: {max(chunk_sizes)} characters\")\n",
    "print(f\"  Std deviation: {np.std(chunk_sizes):.1f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Visualization 2: Embedding Similarity Matrix\n",
    "\n",
    "Let's visualize the similarity between different chunks using their embeddings.\n",
    "This helps us understand how semantically related our documents are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for all chunks\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "chunk_embeddings = embeddings.embed_documents(chunk_texts)\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(chunk_embeddings)\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(similarity_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Cosine Similarity', rotation=270, labelpad=20)\n",
    "\n",
    "# Set labels\n",
    "ax.set_xticks(range(len(chunks)))\n",
    "ax.set_yticks(range(len(chunks)))\n",
    "ax.set_xticklabels([f'Chunk {i}' for i in range(len(chunks))], rotation=45, ha='right')\n",
    "ax.set_yticklabels([f'Chunk {i}' for i in range(len(chunks))])\n",
    "ax.set_title('Cosine Similarity Matrix Between Chunks', fontsize=14, pad=20)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(chunks)):\n",
    "    for j in range(len(chunks)):\n",
    "        text = ax.text(j, i, f'{similarity_matrix[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print most similar chunk pairs\n",
    "print(\"\\nMost similar chunk pairs:\")\n",
    "for i in range(len(chunks)):\n",
    "    for j in range(i+1, len(chunks)):\n",
    "        sim = similarity_matrix[i, j]\n",
    "        if sim > 0.7:  # Threshold for high similarity\n",
    "            print(f\"  Chunk {i} <-> Chunk {j}: {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Visualization 3: Query-Chunk Similarity\n",
    "\n",
    "Let's see which chunks are retrieved for different queries and their similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple queries\n",
    "test_queries = [\n",
    "    \"Who is Mr. Jonas?\",\n",
    "    \"What Jenny's toast said?\",\n",
    "    \"Which object the Mayor Luigi used to try to stop the Spaghetti rain?\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_queries), 1, figsize=(12, 4 * len(test_queries)))\n",
    "\n",
    "for idx, query in enumerate(test_queries):\n",
    "    # Get query embedding\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    \n",
    "    # Calculate similarities with all chunks\n",
    "    similarities = [cosine_similarity([query_embedding], [emb])[0][0] \n",
    "                   for emb in chunk_embeddings]\n",
    "    \n",
    "    # Get top k chunks\n",
    "    top_k_indices = np.argsort(similarities)[-3:][::-1]\n",
    "    top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[idx]\n",
    "    colors = ['green' if i in top_k_indices else 'lightgray' for i in range(len(chunks))]\n",
    "    bars = ax.barh(range(len(chunks)), similarities, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(chunks)))\n",
    "    ax.set_yticklabels([f'Chunk {i}' for i in range(len(chunks))])\n",
    "    ax.set_xlabel('Cosine Similarity Score')\n",
    "    ax.set_title(f'Query: \"{query}\"\\n(Retrieved chunks in green)', fontsize=12)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add similarity values on bars\n",
    "    for i, (bar, sim) in enumerate(zip(bars, similarities)):\n",
    "        if i in top_k_indices:\n",
    "            ax.text(sim + 0.02, i, f'{sim:.3f}', va='center', fontweight='bold')\n",
    "        else:\n",
    "            ax.text(sim + 0.02, i, f'{sim:.3f}', va='center', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed retrieval results\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Retrieve using the retriever\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"\\nRetrieved Chunk {i+1} (similarity score shown in plot above):\")\n",
    "        print(f\"  Content: {doc.page_content[:150]}...\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Visualization 4: 2D Embedding Projection\n",
    "\n",
    "Let's project the high-dimensional embeddings into 2D space using PCA to visualize\n",
    "the relationships between chunks and queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project embeddings to 2D using PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(chunk_embeddings)\n",
    "\n",
    "# Create a query and get its embedding\n",
    "query = \"Who is Mr. Jonas?\"\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "query_2d = pca.transform([query_embedding])[0]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot chunk embeddings\n",
    "scatter = ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                    s=200, c=range(len(chunks)), cmap='viridis', \n",
    "                    alpha=0.7, edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels for chunks\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    ax.annotate(f'Chunk {i}', (x, y), xytext=(5, 5), \n",
    "               textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot query embedding\n",
    "ax.scatter(query_2d[0], query_2d[1], s=300, c='red', marker='*', \n",
    "          edgecolors='darkred', linewidth=2, label='Query', zorder=5)\n",
    "ax.annotate('Query', (query_2d[0], query_2d[1]), xytext=(10, 10),\n",
    "           textcoords='offset points', fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "# Draw lines from query to top 3 chunks\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "for doc in retrieved_docs:\n",
    "    # Find chunk index\n",
    "    chunk_idx = next(i for i, chunk in enumerate(chunks) \n",
    "                    if chunk.page_content == doc.page_content)\n",
    "    ax.plot([query_2d[0], embeddings_2d[chunk_idx, 0]],\n",
    "           [query_2d[1], embeddings_2d[chunk_idx, 1]],\n",
    "           'r--', alpha=0.3, linewidth=2)\n",
    "\n",
    "ax.set_xlabel(f'PC1 (explains {pca.explained_variance_ratio_[0]:.1%} of variance)')\n",
    "ax.set_ylabel(f'PC2 (explains {pca.explained_variance_ratio_[1]:.1%} of variance)')\n",
    "ax.set_title('2D PCA Projection of Chunk Embeddings\\n(Red lines connect query to retrieved chunks)', \n",
    "            fontsize=14, pad=20)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Chunk Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "print(\"(Note: 2D projection may not capture all relationships in high-dimensional space)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate RAG Performance\n",
    "\n",
    "**Task**: \n",
    "1. Create a set of test questions and expected answers\n",
    "2. Implement basic evaluation metrics:\n",
    "   - Retrieval accuracy (are the right chunks retrieved?)\n",
    "   - Answer quality (does the answer contain correct information?)\n",
    "3. Visualize the evaluation results\n",
    "\n",
    "**Questions to answer**:\n",
    "- How do you measure RAG system performance?\n",
    "- What metrics are most important for your use case?\n",
    "- How can you improve the system based on evaluation results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New test dataset based on the whimsical documents\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"Who got promoted in the office?\",\n",
    "        \"expected_keywords\": [\"Mr. Jonas\", \"promotion\", \"supervising\"],\n",
    "        \"expected_chunk_keywords\": [\"Mr. Jonas\", \"promotion\", \"office naps\"],\n",
    "        \"category\": \"character\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What unusual feature does Jenny's toaster have?\",\n",
    "        \"expected_keywords\": [\"time machine\", \"messages from the future\", \"lever\"],\n",
    "        \"expected_chunk_keywords\": [\"toaster\", \"time machine\", \"messages\"],\n",
    "        \"category\": \"object\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What fell from the sky in Noodleton?\",\n",
    "        \"expected_keywords\": [\"spaghetti\", \"rain\", \"noodle\"],\n",
    "        \"expected_chunk_keywords\": [\"spaghetti\", \"rained\", \"town of Noodleton\"],\n",
    "        \"category\": \"event\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What events involve animals in the stories?\",\n",
    "        \"expected_keywords\": [\"cat breaks\", \"squirrels waltzing\", \"Max chasing spaghetti\"],\n",
    "        \"expected_chunk_keywords\": [\"Mr. Jonas\", \"squirrels\", \"Max\"],\n",
    "        \"category\": \"animal\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What unusual food creations happened in Noodleton?\",\n",
    "        \"expected_keywords\": [\"noodle ice cream\", \"spaghetti smoothies\", \"pasta hats\"],\n",
    "        \"expected_chunk_keywords\": [\"chefs\", \"sky-spaghetti recipes\", \"pasta\"],\n",
    "        \"category\": \"food\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use the extended QA chain if available\n",
    "qa_for_eval = qa\n",
    "retriever_for_eval = retriever\n",
    "chunks_for_eval = chunks\n",
    "\n",
    "# The evaluation functions remain the same\n",
    "def evaluate_retrieval(retriever, test_cases, chunks_list):\n",
    "    results = []\n",
    "    for test_case in test_cases:\n",
    "        retrieved = retriever.invoke(test_case[\"question\"])\n",
    "        retrieved_texts = [doc.page_content.lower() for doc in retrieved]\n",
    "        keywords_found_in_chunks = [\n",
    "            keyword for keyword in test_case.get(\"expected_chunk_keywords\", [])\n",
    "            if any(keyword.lower() in text for text in retrieved_texts)\n",
    "        ]\n",
    "        relevance_score = len(keywords_found_in_chunks) / max(len(test_case.get(\"expected_chunk_keywords\", [\"\"])), 1)\n",
    "        results.append({\n",
    "            \"question\": test_case[\"question\"],\n",
    "            \"num_retrieved\": len(retrieved),\n",
    "            \"keywords_found\": keywords_found_in_chunks,\n",
    "            \"relevance_score\": relevance_score,\n",
    "            \"retrieved_chunks\": [doc.page_content[:100] + \"...\" for doc in retrieved]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def evaluate_answers(qa_chain, test_cases):\n",
    "    results = []\n",
    "    for test_case in test_cases:\n",
    "        try:\n",
    "            answer = qa_chain.invoke({\"input\": test_case[\"question\"]})\n",
    "            answer_text = answer[\"result\"].lower()\n",
    "            keywords_found = [kw for kw in test_case[\"expected_keywords\"] if kw.lower() in answer_text]\n",
    "            keyword_coverage = len(keywords_found) / len(test_case[\"expected_keywords\"]) if test_case[\"expected_keywords\"] else 0\n",
    "            answer_length = len(answer[\"result\"])\n",
    "            results.append({\n",
    "                \"question\": test_case[\"question\"],\n",
    "                \"answer\": answer[\"result\"],\n",
    "                \"keywords_found\": keywords_found,\n",
    "                \"keyword_coverage\": keyword_coverage,\n",
    "                \"answer_length\": answer_length,\n",
    "                \"category\": test_case.get(\"category\", \"unknown\")\n",
    "            })\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"question\": test_case[\"question\"],\n",
    "                \"answer\": f\"Error: {e}\",\n",
    "                \"keywords_found\": [],\n",
    "                \"keyword_coverage\": 0,\n",
    "                \"answer_length\": 0,\n",
    "                \"category\": test_case.get(\"category\", \"unknown\")\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# Run evaluations\n",
    "print(\"\\nEvaluating Retrieval Performance...\")\n",
    "print(\"-\" * 60)\n",
    "retrieval_results = evaluate_retrieval(retriever_for_eval, test_cases, chunks_for_eval)\n",
    "\n",
    "print(\"\\nEvaluating Answer Quality...\")\n",
    "print(\"-\" * 60)\n",
    "answer_results = evaluate_answers(qa_for_eval, test_cases)\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Detailed Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (ret_result, ans_result) in enumerate(zip(retrieval_results, answer_results)):\n",
    "    print(f\"\\nTest Case {i+1}: {ret_result['question']}\")\n",
    "    print(f\"  Category: {ans_result.get('category', 'unknown')}\")\n",
    "    print(f\"  Retrieved chunks: {ret_result['num_retrieved']}\")\n",
    "    print(f\"  Retrieval relevance score: {ret_result['relevance_score']:.2%}\")\n",
    "    print(f\"  Keywords found in chunks: {ret_result['keywords_found']}\")\n",
    "    print(f\"  Keywords found in answer: {ans_result['keywords_found']}\")\n",
    "    print(f\"  Keyword coverage: {ans_result['keyword_coverage']:.2%}\")\n",
    "    print(f\"  Answer length: {ans_result['answer_length']} characters\")\n",
    "\n",
    "# Aggregate metrics\n",
    "import numpy as np\n",
    "avg_relevance = np.mean([r[\"relevance_score\"] for r in retrieval_results])\n",
    "avg_coverage = np.mean([r[\"keyword_coverage\"] for r in answer_results])\n",
    "avg_answer_length = np.mean([r[\"answer_length\"] for r in answer_results])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Aggregate Metrics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Average retrieval relevance score: {avg_relevance:.2%}\")\n",
    "print(f\"Average keyword coverage: {avg_coverage:.2%}\")\n",
    "print(f\"Average answer length: {avg_answer_length:.1f} characters\")\n",
    "print(f\"Total test cases: {len(test_cases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Simple RAG Chatbot\n",
    "\n",
    "**Task**: \n",
    "1. Create an interactive chatbot that uses the RAG system\n",
    "2. Implement conversation history (optional: use LangChain's memory)\n",
    "3. Add error handling and user-friendly responses\n",
    "\n",
    "**Questions to answer**:\n",
    "- How do you handle follow-up questions in a RAG system?\n",
    "- Should conversation history be included in the retrieval context?\n",
    "- How can you improve the user experience?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_chatbot(qa_chain, max_turns=10, conversation_history=None):\n",
    "    if conversation_history is None:\n",
    "        conversation_history = []\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"RAG Chatbot - Type 'quit' to exit, 'history' to see conversation\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"This chatbot uses RAG to answer questions based on the document knowledge base.\\n\")\n",
    "    \n",
    "    turns = 0\n",
    "    while turns < max_turns:\n",
    "        try:\n",
    "            question = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'bye', 'q']:\n",
    "                print(\"\\nGoodbye! Thanks for using the RAG chatbot!\")\n",
    "                break\n",
    "            \n",
    "            if question.lower() == 'history':\n",
    "                print(\"\\n\" + \"=\" * 60)\n",
    "                print(\"Conversation History:\")\n",
    "                print(\"=\" * 60)\n",
    "                for i, (q, a) in enumerate(conversation_history, 1):\n",
    "                    print(f\"\\nTurn {i}:\")\n",
    "                    print(f\"  You: {q}\")\n",
    "                    print(f\"  Bot: {a[:200]}...\" if len(a) > 200 else f\"  Bot: {a}\")\n",
    "                continue\n",
    "            \n",
    "            if not question:\n",
    "                print(\"Please enter a question.\")\n",
    "                continue\n",
    "            \n",
    "            print(\"\\nBot: Thinking...\")\n",
    "            answer = qa_chain.invoke({\"input\": question})\n",
    "            answer_text = answer[\"answer\"]\n",
    "            \n",
    "            print(f\"Bot: {answer_text}\")\n",
    "            conversation_history.append((question, answer_text))\n",
    "            turns += 1\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nInterrupted by user. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(\"Please try again or type 'quit' to exit.\")\n",
    "    \n",
    "    return conversation_history\n",
    "\n",
    "# Demo mode: simple test questions\n",
    "# \"Tell me about Mr. Jonas.\",\n",
    "# \"What happens with Jenny's toaster?\",\n",
    "# \"Why is Noodleton famous now?\"\n",
    "\n",
    "conversation = simple_rag_chatbot(qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **RAG Fundamentals**: How retrieval-augmented generation combines document retrieval with language model generation\n",
    "2. **Pipeline Components**: Document loading, chunking, embedding, vector storage, retrieval, and generation\n",
    "3. **LangChain Implementation**: How to build a RAG system using LangChain components\n",
    "4. **Visualization**: How to visualize and analyze different aspects of the RAG pipeline\n",
    "5. **Best Practices**: Chunk sizing, retrieval strategies, and chain types\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "- Explore advanced RAG techniques (reranking, query expansion, hybrid search)\n",
    "- Learn about different embedding models and their trade-offs\n",
    "- Experiment with different vector databases (ChromaDB, Pinecone, Weaviate)\n",
    "- Implement more sophisticated retrieval strategies (BM25 + semantic search)\n",
    "- Build production-ready RAG applications with error handling and monitoring\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [Qdrant Documentation](https://github.com/facebookresearch/qdrant)\n",
    "- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [RAG Research Paper](https://arxiv.org/abs/2005.11401)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
